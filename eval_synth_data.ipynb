{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from core.raft_stereo import RAFTStereo\n",
    "except ImportError:\n",
    "    import os\n",
    "    os.chdir(\"/RAFT-Stereo\")\n",
    "    from core.raft_stereo import RAFTStereo\n",
    "    \n",
    "FRPASS = \"frames_cleanpass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_fusion.dataloader import StereoDataset, StereoDatasetArgs\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusion_args import FusionArgs\n",
    "args = FusionArgs()\n",
    "args.hidden_dims = [128, 128, 128]\n",
    "args.corr_levels = 4\n",
    "args.corr_radius = 4\n",
    "args.n_downsample = 3\n",
    "args.context_norm = \"batch\"\n",
    "args.n_gru_layers = 2\n",
    "args.shared_backbone = True\n",
    "args.mixed_precision = True\n",
    "args.corr_implementation = \"reg_cuda\"\n",
    "args.slow_fast_gru = False\n",
    "args.restore_ckpt = \"models/raftstereo-realtime.pth\"\n",
    "\n",
    "\n",
    "args.lr = 0.001\n",
    "args.train_iters = 7\n",
    "args.valid_iters = 12\n",
    "args.wdecay = 0.0001\n",
    "args.num_steps = 100000\n",
    "args.valid_steps = 1000\n",
    "args.name = \"StereoFusion\"\n",
    "args.batch_size = 4\n",
    "args.fusion = \"AFF\"\n",
    "args.shared_fusion = True\n",
    "args.freeze_backbone = []\n",
    "args.both_side_train= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StereoDataset(\n",
    "    StereoDatasetArgs(\n",
    "        \"/bean/depth\",\n",
    "        flying3d_json=True,\n",
    "        flow3d_driving_json=False,\n",
    "        gt_depth=True,\n",
    "        validate_json=True,\n",
    "        synth_no_filter=True,\n",
    "    )\n",
    ")\n",
    "valid_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(RAFTStereo(args)).cuda()\n",
    "model.load_state_dict(torch.load(args.restore_ckpt))\n",
    "model.eval()\n",
    "model = model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from train_fusion.noise_generator import NoiseGenerator\n",
    "\n",
    "iterator = iter(valid_loader)\n",
    "inputs = next(iterator)\n",
    "image0, image1, image2, image3, dis1, dis2 = [x.cuda() for x in inputs[1:]]\n",
    "image_path = inputs[0][0][0][0]\n",
    "disparity_path_color = [\n",
    "    image_path.replace(\"frames_cleanpass\", f\"flow_raft_rgb_env_color_{i}\")\n",
    "    for i in range(4)\n",
    "]\n",
    "disparity_rgb = [cv2.imread(x) for x in disparity_path_color]\n",
    "image_path_nir = [\n",
    "    image_path.replace(FRPASS, f\"flow_raft_nir_env_color_{i}\")\n",
    "    for i in range(4)\n",
    "]\n",
    "disparity_nir = [cv2.imread(x) for x in image_path_nir]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "\n",
    "o_img_l, o_img_r = cv2.imread(image_path), cv2.imread(\n",
    "    image_path.replace(\"left\", \"right\")\n",
    ")\n",
    "\n",
    "noise = NoiseGenerator()\n",
    "\n",
    "\n",
    "al_left = noise.all_noise(o_img_l)\n",
    "al_right = noise.all_noise(o_img_r)\n",
    "image_dict = {\n",
    "    \"origin\": (o_img_l, o_img_r),\n",
    "    \"noised\": (al_left, al_right),\n",
    "    # \"read_noise\": (noise.add_read_noise(o_img_l), noise.add_read_noise(o_img_r)),\n",
    "    # \"read_noise_std20\": (noise.add_read_noise(o_img_l, std_dev=20), noise.add_read_noise(o_img_r, std_dev=20)),\n",
    "    # \"read_noise_std40\": (noise.add_read_noise(o_img_l, std_dev=40), noise.add_read_noise(o_img_r, std_dev=40)),\n",
    "    # \"shot_noise\": (noise.add_shot_noise(o_img_l), noise.add_shot_noise(o_img_r)),\n",
    "    # \"thermal_noise_std20\": (noise.add_thermal_noise(o_img_l, std_dev=20), noise.add_thermal_noise(o_img_r, std_dev=20)),\n",
    "    # \"thermal_noise_std40\": (noise.add_thermal_noise(o_img_l, std_dev=40), noise.add_thermal_noise(o_img_r, std_dev=40)),\n",
    "    # \"dark_current_noise\": (noise.add_dark_current_noise(o_img_l), noise.add_dark_current_noise(o_img_r)),\n",
    "    # \"quantization_noise\": (noise.add_quantization_noise(o_img_l), noise.add_quantization_noise(o_img_r)),\n",
    "    # \"fixed_pattern_noise\": (noise.add_fixed_pattern_noise(o_img_l), noise.add_fixed_pattern_noise(o_img_r)),\n",
    "    # \"burned_image_64\": (\n",
    "    #     noise.larger_intensity(al_left, 64, 224),\n",
    "    #     noise.larger_intensity(al_right, 64, 224),\n",
    "    # ),\n",
    "    # \"burned_image_64128\": (\n",
    "    #     noise.larger_intensity(al_left, 64, 128),\n",
    "    #     noise.larger_intensity(al_right, 64, 128),\n",
    "    # ),\n",
    "    # \"burned_image_64192\": (\n",
    "    #     noise.larger_intensity(al_left, 64, 192),\n",
    "    #     noise.larger_intensity(al_right, 64, 192),\n",
    "    # ),\n",
    "    # \"burned_image_96\": (\n",
    "    #     noise.larger_intensity(al_left, 96, 224),\n",
    "    #     noise.larger_intensity(al_right, 96, 224),\n",
    "    # ),\n",
    "    # \"burned_image_128\": (\n",
    "    #     noise.larger_intensity(al_left, 128, 224),\n",
    "    #     noise.larger_intensity(al_right, 128, 224),\n",
    "    # ),\n",
    "    # \"all_noise_darker_beta_50_gamma01\": (\n",
    "    #     noise.darker_image(o_img_l, beta=50, gamma=0.1),\n",
    "    #     noise.darker_image(o_img_r, beta=50, gamma=0.1),\n",
    "    # ),\n",
    "    # \"all_noise_darker_beta_50_gamma02\": (\n",
    "    #     noise.darker_image(o_img_l, beta=50, gamma=0.2),\n",
    "    #     noise.darker_image(o_img_r, beta=50, gamma=0.2),\n",
    "    # ),\n",
    "    # \"all_noise_darker_burned_64\": (\n",
    "    #     noise.darker_image(noise.larger_intensity(al_left, 64, 224), beta=50),\n",
    "    #     noise.darker_image(noise.larger_intensity(al_right, 64, 224), beta=50),\n",
    "    # ),\n",
    "    # \"all_noise_darker_burned_64128\": (\n",
    "    #     noise.darker_image(noise.larger_intensity(al_left, 64, 128), beta=50),\n",
    "    #     noise.darker_image(noise.larger_intensity(al_right, 64, 128), beta=50),\n",
    "    # ),\n",
    "    \"burned_v2_image_64100\" :(\n",
    "        noise.burnt_effect(o_img_l, 64, 100),\n",
    "        noise.burnt_effect(o_img_r, 64, 100),\n",
    "    ),\n",
    "    \"burned_v2_image_64500\" :(\n",
    "        noise.burnt_effect(o_img_l,64, 500),\n",
    "        noise.burnt_effect(o_img_r, 64, 500),\n",
    "    ),\n",
    "    \"burned_v2_image_641000\" :(\n",
    "        noise.burnt_effect(o_img_l, 64, 1000),\n",
    "        noise.burnt_effect(o_img_r, 64, 1000),\n",
    "    ),\n",
    "    \"noised_burned_v2_image_64100\" :(\n",
    "        noise.burnt_effect(al_left, 64, 100),\n",
    "        noise.burnt_effect(al_right, 64, 100),\n",
    "    ),\n",
    "    \"noised_burned_v2_image_64500\" :(\n",
    "        noise.burnt_effect(al_left,64, 500),\n",
    "        noise.burnt_effect(al_right, 64, 500),\n",
    "    ),\n",
    "    \"noised_burned_v2_image_641000\" :(\n",
    "        noise.burnt_effect(al_left, 64, 1000),\n",
    "        noise.burnt_effect(al_right, 64, 1000),\n",
    "    ),\n",
    "    \"darker_shadow\":(\n",
    "        noise.darken_shadows(o_img_l, 64, 500),\n",
    "        noise.darken_shadows(o_img_r, 64, 500),\n",
    "    ),\n",
    "    \"darker_shadow_burnt\":(\n",
    "        noise.darken_shadows(noise.burnt_effect(o_img_l,64, 500), 64, 500),\n",
    "        noise.darken_shadows(noise.burnt_effect(o_img_r,64, 500), 64, 500),\n",
    "    ),\n",
    "    # \"darker_bunred_200\": (\n",
    "    #      noise.darker_image(noise.burnt_effect(o_img_l, 64, 500), beta=50),\n",
    "    #      noise.darker_image(noise.burnt_effect(o_img_r, 64, 500), beta=50),\n",
    "    #  ),\n",
    "    # \"darker_bunred_150\": (\n",
    "    #     noise.darker_image(noise.burnt_effect(o_img_l, 96, 500), beta=50),\n",
    "    #     noise.darker_image(noise.burnt_effect(o_img_r, 96, 500), beta=50),\n",
    "    # ),\n",
    "    # \"darker_bunred_128\": (\n",
    "    #     noise.darker_image(noise.larger_intensity(o_img_l, 128, 224), beta=50),\n",
    "    #     noise.darker_image(noise.larger_intensity(o_img_r, 128, 224), beta=50),\n",
    "    # ),\n",
    "    # \"all_noise\": (all_noise(o_img_l), all_noise(o_img_r)),\n",
    "    # \"all_noise_darker\": (darker_image(all_noise(o_img_l)), darker_image(all_noise(o_img_r))),\n",
    "    # \"all_noise_darker_alpha_2\": (darker_image(all_noise(o_img_l), alpha=2.0), darker_image(all_noise(o_img_r), alpha=2.0)),\n",
    "    # \"all_noise_darker_alpha_3\": (darker_image(all_noise(o_img_l), alpha=3.0), darker_image(all_noise(o_img_r), alpha=3.0)),\n",
    "    # \"all_noise_darker_beta_50\": (darker_image(all_noise(o_img_l), beta=50), darker_image(all_noise(o_img_r), beta=50)),\n",
    "    # \"all_noise_darker_beta_100\": (darker_image(all_noise(o_img_l), beta=100), darker_image(all_noise(o_img_r), beta=100)),\n",
    "    # \"all_noise_darker_alpha_2_beta_50\": (darker_image(all_noise(o_img_l), alpha=2.0, beta=50), darker_image(all_noise(o_img_r), alpha=2.0, beta=50)),\n",
    "    # \"all_noise_darker_alpha_3_beta_50\": (darker_image(all_noise(o_img_l), alpha=3.0, beta=50), darker_image(all_noise(o_img_r), alpha=3.0, beta=50)),\n",
    "    # \"all_noise_darker_alpha_2_beta_100\": (darker_image(all_noise(o_img_l), alpha=2.0, beta=100), darker_image(all_noise(o_img_r), alpha=2.0, beta=100)),\n",
    "    # \"all_noise_darker_alpha_3_beta_100\": (darker_image(all_noise(o_img_l), alpha=3.0, beta=100), darker_image(all_noise(o_img_r), alpha=3.0, beta=100)),\n",
    "    \"nir_rendered\": (image2[0], image3[0]),\n",
    "    \"nir_ambient\": (image2[1], image3[1]),\n",
    "    \n",
    "}\n",
    "\n",
    "_, axs = plt.subplots(\n",
    "    len(image_dict.keys()), 4, figsize=(20, 5 * len(image_dict.keys()))\n",
    ")\n",
    "for i, (key, image) in enumerate(image_dict.items()):\n",
    "    DISP_MAX = 128\n",
    "    DISP_MIN = 16\n",
    "    left, right = image\n",
    "    if type(left) == np.ndarray:\n",
    "        left = torch.from_numpy(left).permute(2, 0, 1).unsqueeze(0).float().cuda()\n",
    "        right = torch.from_numpy(right).permute(2, 0, 1).unsqueeze(0).float().cuda()\n",
    "    else:\n",
    "        left = left.unsqueeze(0).float()\n",
    "        right = right.unsqueeze(0).float()\n",
    "    if left.shape[1] == 1:\n",
    "        left = torch.cat([left, left, left], dim=1)\n",
    "        right = torch.cat([right, right, right], dim=1)\n",
    "    with torch.no_grad():\n",
    "        _, flow = model(left, right, iters=args.valid_iters, test_mode=True)\n",
    "    flow = -flow[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    axs[i, 0].imshow(left[0].permute(1, 2, 0).cpu().numpy().astype(np.uint8))\n",
    "    axs[i, 0].set_title(f\"{key} Left Image\")\n",
    "    axs[i, 1].imshow(right[0].permute(1, 2, 0).cpu().numpy().astype(np.uint8))\n",
    "    axs[i, 1].set_title(f\"{key} Right Image\")\n",
    "    et = axs[i, 2].imshow(flow, vmin=DISP_MIN, vmax=DISP_MAX, cmap=\"magma\")\n",
    "    axs[i, 2].set_title(f\"{key} Flow\")\n",
    "    axs[i, 2].axis(\"off\")\n",
    "    plt.colorbar(et, ax=axs[i, 2])\n",
    "\n",
    "    disparity_gt = dis1[0].permute(1, 2, 0).cpu().numpy()\n",
    "    flow = flow[: disparity_gt.shape[0], : disparity_gt.shape[1]]\n",
    "\n",
    "    error = noise.compute_disparity_gt_error(disparity_gt, flow, False)\n",
    "\n",
    "    et = axs[i, 3].imshow(255 - error, vmin=0, vmax=255, cmap=\"jet\")\n",
    "    axs[i, 3].set_title(f\"{key} GT Error\")\n",
    "    axs[i, 3].axis(\"off\")\n",
    "    plt.colorbar(et, ax=axs[i, 3])\n",
    "\n",
    "    # et = axs[i, 4].imshow(disparity_gt, vmin=DISP_MIN, vmax=DISP_MAX, cmap=\"magma\")\n",
    "    # axs[i, 4].set_title(f\"{key} Gt\")\n",
    "    # axs[i, 4].axis(\"off\")\n",
    "    # plt.colorbar(et, ax=axs[i, 4])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis\n",
    "import json\n",
    "from typing import List\n",
    "import cv2\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from train_fusion.loss_function import warp_reproject_loss, gt_loss\n",
    "from train_fusion.noise_generator import NoiseGenerator\n",
    "\n",
    "noise = NoiseGenerator()\n",
    "\n",
    "\n",
    "batch_len = len(valid_loader)\n",
    "\n",
    "input_title = [\n",
    "    \"rgb\",\n",
    "    \"burnt\",\n",
    "    \"burnt_light\",\n",
    "    \"darken\",\n",
    "    \"darken_gain\",\n",
    "    \"nir_rendered\",\n",
    "\n",
    "]\n",
    "mode_len = len(input_title)\n",
    "total_loss_dict = [{} for _ in range(mode_len)]\n",
    "\n",
    "\n",
    "def numpy_to_torch(imgs: list[np.ndarray]):\n",
    "    return [torch.from_numpy(img).float().cuda().permute(2, 0, 1) for img in imgs]\n",
    "\n",
    "\n",
    "def rgb_noised_input_pairs(rgb_left: np.ndarray, rgb_right: np.ndarray):\n",
    "    (\n",
    "        rgb_left,\n",
    "        rgb_right,\n",
    "        rgb_burnt_left,\n",
    "        rgb_burnt_right,\n",
    "        rgb_light_left,\n",
    "        rgb_light_right,\n",
    "        rgb_darken_left,\n",
    "        rgb_darken_right,\n",
    "        rgb_darken_gain_left,\n",
    "        rgb_darken_gain_right,\n",
    "    ) = numpy_to_torch(\n",
    "        [\n",
    "            rgb_left,\n",
    "            rgb_right,\n",
    "            noise.filter_image_burn(rgb_left),\n",
    "            noise.filter_image_burn(rgb_right),\n",
    "            noise.filter_image_burn_light(rgb_left),\n",
    "            noise.filter_image_burn_light(rgb_right),\n",
    "            noise.filter_image_dark(rgb_left),\n",
    "            noise.filter_image_dark(rgb_right),\n",
    "            noise.filter_image_dark_high_gain(rgb_left),\n",
    "            noise.filter_image_dark_high_gain(rgb_right),\n",
    "        ]\n",
    "    )\n",
    "    left_arr = torch.stack(\n",
    "        [\n",
    "            rgb_left,\n",
    "            rgb_burnt_left,\n",
    "            rgb_light_left,\n",
    "            rgb_darken_left,\n",
    "            rgb_darken_gain_left,\n",
    "        ]\n",
    "    )\n",
    "    right_arr = torch.stack(\n",
    "        [\n",
    "            rgb_right,\n",
    "            rgb_burnt_right,\n",
    "            rgb_light_right,\n",
    "            rgb_darken_right,\n",
    "            rgb_darken_gain_right,\n",
    "        ]\n",
    "    )\n",
    "    return left_arr, right_arr\n",
    "\n",
    "\n",
    "def unpack_batch_create_pair_arr(img_cuda: List[torch.Tensor]):\n",
    "    img_cuda[2] = img_cuda[2].repeat(1, 3, 1, 1)\n",
    "    img_cuda[3] = img_cuda[3].repeat(1, 3, 1, 1)\n",
    "\n",
    "    rgb_left = img_cuda[0][0].permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "    rgb_right = img_cuda[1][0].permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "    nir_rendered_left = img_cuda[2][0]\n",
    "    nir_rendered_right = img_cuda[3][0]\n",
    "    left_arr, right_arr = rgb_noised_input_pairs(rgb_left, rgb_right)\n",
    "    left_arr = torch.cat(\n",
    "        [left_arr, nir_rendered_left.unsqueeze(0)]\n",
    "    )\n",
    "    right_arr = torch.cat(\n",
    "        [right_arr, nir_rendered_right.unsqueeze(0)]\n",
    "    )\n",
    "    return left_arr, right_arr\n",
    "\n",
    "\n",
    "def compute_absolute_error(flow, flow_gt):\n",
    "    h, w = flow.shape[-2:]\n",
    "    hf = int(h / 2 - h / 4)\n",
    "    ht = int(h / 2 + h / 4)\n",
    "    wf = int(w / 2 - w / 4)\n",
    "    wt = int(w / 2 + w / 4)\n",
    "\n",
    "    rmse_rgb = torch.sqrt(torch.mean((-_flow - flow_gt)[:, :, hf:ht, wf:wt] ** 2))\n",
    "\n",
    "    mae_rgb = torch.mean(torch.abs(-_flow - flow_gt)[:, :, hf:ht, wf:wt])\n",
    "\n",
    "    ard_rgb = torch.mean(\n",
    "        torch.abs(-_flow - flow_gt)[:, :, hf:ht, wf:wt] / flow_gt[:, :, hf:ht, wf:wt]\n",
    "    )\n",
    "    return {\n",
    "        \"rmse\": rmse_rgb.item(),\n",
    "        \"mae\": mae_rgb.item(),\n",
    "        \"ard\": ard_rgb.item(),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_disparity(image_path: str, disparity: np.ndarray):\n",
    "    disparity_path_npz = image_path.replace(\n",
    "        FRPASS, f\"disparity_raft_env_{env_title}\"\n",
    "    ).replace(\"png\", \"npz\")\n",
    "    os.makedirs(os.path.dirname(disparity_path_npz), exist_ok=True)\n",
    "    np.savez_compressed(disparity_path_npz, disparity=disparity)\n",
    "    disparity_path_color = image_path.replace(\n",
    "        FRPASS, f\"disparity_raft_env_color_{env_title}\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(disparity_path_color), exist_ok=True)\n",
    "\n",
    "    disparity_color = cv2.applyColorMap(\n",
    "        np.clip(disparity, 0, 256).astype(np.uint8), cv2.COLORMAP_MAGMA\n",
    "    )\n",
    "\n",
    "    cv2.imwrite(disparity_path_color, disparity_color)\n",
    "\n",
    "\n",
    "def save_image_pair(\n",
    "    image_path_rgb: str, left_tensor: torch.Tensor, right_tensor: torch.Tensor\n",
    "):\n",
    "    os.makedirs(os.path.dirname(image_path_rgb), exist_ok=True)\n",
    "    image_left = left_tensor.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "    print(image_path_rgb, image_left.shape)\n",
    "    cv2.imwrite(image_path_rgb, image_left)\n",
    "    image_path_rgb = image_path_rgb.replace(\"left\", \"right\")\n",
    "    os.makedirs(os.path.dirname(image_path_rgb), exist_ok=True)\n",
    "    image_right = right_tensor.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "    cv2.imwrite(image_path_rgb, image_right)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, input_valid in enumerate(tqdm(valid_loader)):\n",
    "\n",
    "    image_list, *blob = input_valid\n",
    "    img_cuda = [img.cuda() for img in blob]\n",
    "    left_arr, right_arr = unpack_batch_create_pair_arr(img_cuda)\n",
    "    image_path = image_list[0][0]\n",
    "    for i in range(mode_len):\n",
    "        env_title = input_title[i]\n",
    "        if not env_title in [\"rgb\", \"nir_rendered\", \"nir_ambient\"]:\n",
    "            save_image_pair(\n",
    "                image_path.replace(\n",
    "                    FRPASS, f\"frame_{env_title}_filtered\"\n",
    "                ),\n",
    "                left_arr[i],\n",
    "                right_arr[i],\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, input_valid in enumerate(tqdm(valid_loader)):\n",
    "    with torch.no_grad():\n",
    "        image_list, *blob = input_valid\n",
    "        img_cuda = [img.cuda() for img in blob]\n",
    "        left_arr, right_arr = unpack_batch_create_pair_arr(img_cuda)\n",
    "\n",
    "        _, flows = model(left_arr, right_arr, iters=args.valid_iters, test_mode=True)\n",
    "\n",
    "        flows = flows[:, :, : img_cuda[0].shape[2], : img_cuda[0].shape[3]]\n",
    "        image_path = image_list[0][0][0]\n",
    "        flow_gt = img_cuda[4][0:1]\n",
    "        for i in range(mode_len):\n",
    "            _flow = flows[i : i + 1]\n",
    "            _, loss_gt = gt_loss(None, flow_gt, [_flow])\n",
    "            _, loss_self = warp_reproject_loss(\n",
    "                [_flow], left_arr[i : i + 1], right_arr[i : i + 1]\n",
    "            )\n",
    "            ab_loss = compute_absolute_error(_flow, flow_gt)\n",
    "            loss = {\n",
    "                **loss_gt,\n",
    "                **loss_self,\n",
    "                **ab_loss,\n",
    "            }\n",
    "\n",
    "            env_title = input_title[i]\n",
    "\n",
    "            disparity = -_flow[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "            for key in loss:\n",
    "                if key not in total_loss_dict[i]:\n",
    "                    total_loss_dict[i][key] = 0\n",
    "                total_loss_dict[i][key] += loss[key] / batch_len\n",
    "            loss_dict_path = image_path.replace(\n",
    "                FRPASS, f\"loss_raft_env_{env_title}\"\n",
    "            ).replace(\".png\", \".json\")\n",
    "\n",
    "            os.makedirs(os.path.dirname(loss_dict_path), exist_ok=True)\n",
    "            json.dump(loss, open(loss_dict_path, \"w\"))\n",
    "\n",
    "            \"\"\"\n",
    "            Save disparity and filtered input image\n",
    "            \"\"\"\n",
    "            save_disparity(image_path, disparity)\n",
    "\n",
    "            if not env_title in [\"rgb\", \"nir_rendered\", \"nir_ambient\"]:\n",
    "                save_image_pair(\n",
    "                    image_path.replace(\n",
    "                        FRPASS, f\"frame_{env_title}_filtered\"\n",
    "                    ),\n",
    "                    left_arr[i],\n",
    "                    right_arr[i],\n",
    "                )\n",
    "\n",
    "\n",
    "print(total_loss_dict)\n",
    "json.dump(total_loss_dict, open(\"loss_raft_env.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_dict = [{}, {}, {}, {}, {}, {}, {}]\n",
    "batch_len = 0\n",
    "for i_batch, input_valid in enumerate(tqdm(valid_loader)):\n",
    "    image_path = image_list[0][0][0]\n",
    "    for i in range(mode_len):\n",
    "        env_title = input_title[i]\n",
    "        loss_dict_path = image_path.replace(\n",
    "                FRPASS, f\"loss_raft_env_{env_title}\"\n",
    "            ).replace(\".png\", \".json\")\n",
    "        if not os.path.exists(loss_dict_path):\n",
    "            batch_len = i_batch\n",
    "            break\n",
    "        loss_dict = json.load(open(loss_dict_path))\n",
    "        for key, value in loss_dict.items():\n",
    "            if key not in total_loss_dict[i]:\n",
    "                total_loss_dict[i][key] = 0\n",
    "            total_loss_dict[i][key] += value\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(mode_len):\n",
    "    for key in total_loss_dict[i]:\n",
    "        total_loss_dict[i][key] /= 1318\n",
    "        \n",
    "print(total_loss_dict)\n",
    "with open(\"loss_raft_env.json\", \"w\") as f:\n",
    "    json.dump(total_loss_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_dict_adjusted = [\n",
    "    {key: value for key, value in loss_dict.items()}\n",
    "    for loss_dict in total_loss_dict\n",
    "]\n",
    "for dict in total_loss_dict_adjusted:\n",
    "    for key in dict:\n",
    "        dict[key] = dict[key]  * 26694/ 4369\n",
    "print (total_loss_dict_adjusted)\n",
    "\n",
    "json.dump(total_loss_dict_adjusted, open(\"loss_raft_env_adjusted3.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dict in total_loss_dict_adjusted:\n",
    "    print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "total_loss_dict_adjusted = json.load(open(\"loss_raft_env_adjusted3.json\"))\n",
    "\n",
    "# 각 dictionary의 키\n",
    "keys = list(total_loss_dict_adjusted[0].keys())\n",
    "keys = [key for key in keys if not \"ssim\" in key and not \"l1\" in key]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, axs = plt.subplots(len(keys), 1, figsize=(10, 30))\n",
    "\n",
    "\n",
    "for i, key in enumerate(keys):\n",
    "    values = [dict[key] for dict in total_loss_dict_adjusted]\n",
    "    print(len(input_title), len(values))\n",
    "    axs[i].bar(input_title[:6], values, width)\n",
    "    axs[i].set_ylabel(key)\n",
    "    axs[i].set_title(key)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def get_scene_list(path: str):\n",
    "    scene_list = [os.path.join(path, x) for x in os.listdir(path) if x.endswith(\".png\")]\n",
    "    scene_list.sort()\n",
    "    return scene_list\n",
    "\n",
    "\n",
    "def get_scene_prop_path(path: str):\n",
    "    path_img_left = path\n",
    "    path_img_right = path.replace(\"left\", \"right\")\n",
    "    path_disparity_gt = path.replace(FRPASS, \"disparity\").replace(\".png\", \".pfm\")\n",
    "    path_dict = {\n",
    "        \"rgb\": (path_img_left, path_img_right),\n",
    "    }\n",
    "    flow_path_dict = {\n",
    "        \n",
    "    }\n",
    "    for title in [\"rgb\", \"burnt\", \"burnt_light\", \"darken\", \"darken_gain\", \"nir_rendered\"]:\n",
    "        if title == \"rgb\" or title == \"nir_ambient\":\n",
    "            continue\n",
    "        if title == \"nir_rendered\":\n",
    "            path_name = path_img_left.replace(FRPASS, \"nir_rendered\")\n",
    "        else:\n",
    "            path_name = path_img_left.replace(FRPASS, f\"frame_{title}_filtered\")\n",
    "        path_dict[title] = (\n",
    "            path_name,\n",
    "            path_name.replace(\"left\", \"right\"),\n",
    "        )\n",
    "    for title in [\"rgb\", \"burnt\", \"burnt_light\", \"darken\", \"darken_gain\", \"nir_rendered\"]:\n",
    "        if title == \"nir_ambient\":\n",
    "            continue\n",
    "        path_name = path.replace(FRPASS, f\"disparity_raft_env_{title}\").replace(\"png\", \"npz\")\n",
    "        flow_path_dict[title] = path_name\n",
    "    return {\n",
    "        \"input\": path_dict,\n",
    "        \"output\": flow_path_dict,\n",
    "        \"gt\": path_disparity_gt,\n",
    "    }\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "folder = \"/bean/flyingthings3d/frames_cleanpass/TRAIN/A/0000/left\"\n",
    "scene_root_list = get_scene_list(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from train_fusion.noise_generator import NoiseGenerator\n",
    "\n",
    "\n",
    "\n",
    "def plot_disparity_grid(scenes, keys = [\"rgb\", \"burnt\", \"burnt_light\", \"darken\", \"darken_gain\", \"nir_rendered\"]):\n",
    "    \"\"\"\n",
    "    scenes: list of dictionaries containing paths to images and titles for each row\n",
    "    \"\"\"\n",
    "    num_rows = len(keys)\n",
    "    \n",
    "    # Define GridSpec with a smaller width ratio for the 3rd column\n",
    "    gs = gridspec.GridSpec(num_rows, 6, width_ratios=[5, 5, 5, 1, 5, 1])\n",
    "    \n",
    "    fig = plt.figure(figsize=(28, 4 * num_rows))\n",
    "\n",
    "    for row_idx, key in enumerate(keys):\n",
    "        # Load images\n",
    "        img_left = scenes[\"input\"][key][0]\n",
    "        img_right = scenes[\"input\"][key][1]\n",
    "        disparity = scenes[\"output\"][key]\n",
    "        gt = scenes[\"gt\"]\n",
    "        print(img_left, img_right, disparity)\n",
    "        disparity = np.load(disparity)[\"disparity\"].squeeze()\n",
    "        \n",
    "        img_left = cv2.imread(img_left)\n",
    "        img_right = cv2.imread(img_right)\n",
    "        disparity_error = NoiseGenerator().compute_disparity_gt_error(gt, disparity)\n",
    "        title = key\n",
    "\n",
    "        # Convert BGR to RGB for correct color display\n",
    "        if len(img_left.shape) == 3 and img_left.shape[2] == 3:\n",
    "            img_left = cv2.cvtColor(img_left, cv2.COLOR_BGR2RGB)\n",
    "        if len(img_right.shape) == 3 and img_right.shape[2] == 3:\n",
    "            img_right = cv2.cvtColor(img_right, cv2.COLOR_BGR2RGB)\n",
    "        if len(disparity.shape) == 3 and disparity.shape[2] == 3:\n",
    "            disparity = cv2.cvtColor(disparity, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Plot images\n",
    "        ax1 = fig.add_subplot(gs[row_idx, 0])\n",
    "        ax1.imshow(img_left, cmap=\"gray\" if img_left.ndim == 2 else None)\n",
    "        ax1.axis(\"off\")\n",
    "        ax1.set_title(\"Left Image\", fontsize=12)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[row_idx, 1])\n",
    "        ax2.imshow(img_right, cmap=\"gray\" if img_right.ndim == 2 else None)\n",
    "        ax2.axis(\"off\")\n",
    "        ax2.set_title(\"Right Image\", fontsize=12)\n",
    "\n",
    "        ax3 = fig.add_subplot(gs[row_idx, 2])\n",
    "        ax3.imshow(disparity, cmap=\"gray\" if disparity.ndim == 2 else None)\n",
    "        ax3.axis(\"off\")\n",
    "        ax3.set_title(\"Disparity\", fontsize=12)\n",
    "\n",
    "        # Add colorbar as an image with adjusted aspect ratio\n",
    "        gradient = np.linspace(0, 1, 256).reshape(-1, 1)\n",
    "        gradient = np.repeat(gradient, 10, axis=1)  # Make it 1/10th the width of its height\n",
    "\n",
    "        ax4 = fig.add_subplot(gs[row_idx, 3])\n",
    "        ax4.imshow(gradient, aspect='auto', cmap='magma', vmin=0, vmax=1)\n",
    "        ax4.set_aspect(1)\n",
    "        ax4.set_yticks(np.linspace(0, 255, num=5))\n",
    "        ax4.set_yticklabels(np.linspace(0, 255, num=5, dtype=int))\n",
    "        ax4.set_xticks([])\n",
    "\n",
    "        # Compute and plot disparity error\n",
    "        \n",
    "        ax5 = fig.add_subplot(gs[row_idx, 4])\n",
    "        ax5.imshow(disparity_error)\n",
    "        ax5.axis(\"off\")\n",
    "        ax5.set_title(\"Gt Error\", fontsize=12)\n",
    "\n",
    "\n",
    "        # Add colorbar as an image with adjusted aspect ratio\n",
    "        gradient2 = np.linspace(0, 1, 256).reshape(-1, 1)\n",
    "        gradient2 = np.repeat(gradient2, 10, axis=1)  # Make it 1/10th the width of its height\n",
    "\n",
    "        ax6 = fig.add_subplot(gs[row_idx, 5])\n",
    "        ax6.imshow(gradient2, aspect='auto', cmap='jet', vmin=0, vmax=1)\n",
    "        ax6.set_aspect(1)\n",
    "        ax6.set_yticks(np.linspace(0, 255, num=5))\n",
    "        ax6.set_yticklabels(np.linspace(0, 4, num=5, dtype=int))\n",
    "        ax6.set_xticks([])\n",
    "        # Add row title\n",
    "        ax1.text(\n",
    "            -0.5,\n",
    "            0.5,\n",
    "            title,\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"center\",\n",
    "            ha=\"right\",\n",
    "            rotation=90,\n",
    "            transform=ax1.transAxes,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(\"disparity_grid.png\")\n",
    "    plt.close()\n",
    "    return cv2.imread(\"disparity_grid.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "scene_root_list = get_scene_list(\"/bean/flyingthings3d/frames_cleanpass/TEST/A/0000/left\")\n",
    "\n",
    "for scene in tqdm(scene_root_list):\n",
    "    scene_prop = get_scene_prop_path(scene)\n",
    "    plot_image = plot_disparity_grid(scene_prop)\n",
    "    output_path = scene.replace(\"frames_cleanpass\", \"plot\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    cv2.imwrite(scene.replace(\"frames_cleanpass\", \"plot\"), plot_image)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "# 비디오 파일로 저장하는 함수\n",
    "def save_video(image_paths: list[str], video_path, fps=2):\n",
    "    height, width = cv2.imread(image_paths[0].replace(\"frames_cleanpass\", \"plot\")).shape[:2]\n",
    "    print(width, height)\n",
    "    # 비디오 파일 쓰기 설정\n",
    "    fourcc = cv2.VideoWriter.fourcc(*'mp4v')  # 코덱 설정\n",
    "    out = cv2.VideoWriter(video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for img in tqdm(image_paths):\n",
    "        img = img.replace(\"frames_cleanpass\", \"plot\")\n",
    "        if not os.path.exists(img):\n",
    "            break\n",
    "        image = cv2.imread(img)\n",
    "        out.write( image)\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"비디오가 저장되었습니다: {video_path}\")\n",
    "\n",
    "# 비디오 파일로 저장\n",
    "video_path = \"output_video.mp4\"\n",
    "save_video(scene_root_list, video_path)\n",
    "\n",
    "# 주피터 노트북에서 비디오 재생\n",
    "display(Video(video_path, embed=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
