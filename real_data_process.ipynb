{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from core.raft_stereo import RAFTStereo\n",
    "except ImportError:\n",
    "    import os\n",
    "    os.chdir(\"/RAFT-Stereo\")\n",
    "    from core.raft_stereo import RAFTStereo\n",
    "    \n",
    "FRPASS = \"frames_cleanpass\"\n",
    "from train_fusion.dataloader import StereoDataset, StereoDatasetArgs\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from fusion_args import FusionArgs\n",
    "args = FusionArgs()\n",
    "args.hidden_dims = [128, 128, 128]\n",
    "args.corr_levels = 4\n",
    "args.corr_radius = 4\n",
    "args.n_downsample = 3\n",
    "args.context_norm = \"batch\"\n",
    "args.n_gru_layers = 2\n",
    "args.shared_backbone = True\n",
    "args.mixed_precision = True\n",
    "args.corr_implementation = \"reg\"\n",
    "args.slow_fast_gru = False\n",
    "args.restore_ckpt = \"models/raftstereo-realtime.pth\"\n",
    "\n",
    "\n",
    "args.lr = 0.001\n",
    "args.train_iters = 7\n",
    "args.valid_iters = 20\n",
    "args.wdecay = 0.0001\n",
    "args.num_steps = 100000\n",
    "args.valid_steps = 1000\n",
    "args.name = \"StereoFusion\"\n",
    "args.batch_size = 4\n",
    "args.fusion = \"AFF\"\n",
    "args.shared_fusion = True\n",
    "args.freeze_backbone = []\n",
    "args.both_side_train= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(RAFTStereo(args)).to(device)\n",
    "model.load_state_dict(torch.load(args.restore_ckpt))\n",
    "model.eval()\n",
    "model = model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "DISPARITY_MAX = 64\n",
    "\n",
    "\n",
    "def cv2img_to_torch(img: np.ndarray):\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = torch.tensor(img).to(device).float().unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_frame_spectral(frame_folder: str):\n",
    "    left = cv2.imread(os.path.join(frame_folder, \"left.png\"))\n",
    "    right = cv2.imread(os.path.join(frame_folder, \"right.png\"))\n",
    "    left = cv2img_to_torch(left)\n",
    "    right = cv2img_to_torch(right)\n",
    "    _, flow = model(left, right, iters=15, test_mode=True)\n",
    "    flow = -flow[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    return flow\n",
    "\n",
    "\n",
    "def store_disparity(\n",
    "    frame_folder: str, channel: str, disp: np.ndarray, DISPARITY_MAX=64\n",
    "):\n",
    "    disparity_color = cv2.applyColorMap(\n",
    "        (np.clip(disp, 0, DISPARITY_MAX) / DISPARITY_MAX * 255.0).astype(np.uint8),\n",
    "        cv2.COLORMAP_MAGMA,\n",
    "    )\n",
    "    cv2.imwrite(os.path.join(frame_folder, channel, \"disparity.png\"), disparity_color)\n",
    "\n",
    "\n",
    "def process_frame(frame_folder: str, overwrite=False):\n",
    "    if (\n",
    "        not overwrite\n",
    "        and os.path.exists(os.path.join(frame_folder, \"rgb\", \"disparity.png\"))\n",
    "        and os.path.exists(os.path.join(frame_folder, \"nir\", \"disparity.png\"))\n",
    "    ):\n",
    "        return\n",
    "    disparity_rgb = process_frame_spectral(frame_folder + \"/rgb\")\n",
    "    disparity_nir = process_frame_spectral(frame_folder + \"/nir\")\n",
    "\n",
    "    disparity_max = max(disparity_rgb.max(), disparity_nir.max()) // 64 * 64\n",
    "    store_disparity(frame_folder, \"rgb\", disparity_rgb, disparity_max)\n",
    "    store_disparity(frame_folder, \"nir\", disparity_nir, disparity_max)\n",
    "    return disparity_rgb, disparity_nir\n",
    "\n",
    "\n",
    "def process_frame_spectral_batch(frame_folder_list: List[str], spectral: str):\n",
    "    left_list = []\n",
    "    right_list = []\n",
    "    for frame_folder in frame_folder_list:\n",
    "        left = cv2.imread(os.path.join(frame_folder, spectral, \"left.png\"))\n",
    "        right = cv2.imread(os.path.join(frame_folder, spectral, \"right.png\"))\n",
    "        left = cv2img_to_torch(left)\n",
    "        right = cv2img_to_torch(right)\n",
    "        left_list.append(left)\n",
    "        right_list.append(right)\n",
    "    left = torch.cat(left_list, 0)\n",
    "    right = torch.cat(right_list, 0)\n",
    "    with torch.no_grad():\n",
    "        _, flow = model(left, right, iters=24, test_mode=True)\n",
    "    flow = -flow[:, 0].cpu().numpy()\n",
    "    return flow\n",
    "\n",
    "\n",
    "def process_frame_batch(frame_folder_list: List[str], overwrite=False):\n",
    "\n",
    "    disparity_rgb = process_frame_spectral_batch(frame_folder_list, \"rgb\")\n",
    "    disparity_nir = process_frame_spectral_batch(frame_folder_list, \"nir\")\n",
    "\n",
    "    disparity_max = (max(disparity_rgb.max(), disparity_nir.max()) + 31) // 32 * 32\n",
    "    for idx, frame_folder in enumerate(frame_folder_list):\n",
    "        store_disparity(frame_folder, \"rgb\", disparity_rgb[idx], disparity_max)\n",
    "        store_disparity(frame_folder, \"nir\", disparity_nir[idx], disparity_max)\n",
    "    return disparity_rgb, disparity_nir\n",
    "\n",
    "\n",
    "def process_scene(folder: str):\n",
    "    frame_folders = [\n",
    "        os.path.join(folder, x)\n",
    "        for x in tqdm(os.listdir(folder))\n",
    "        if x.split(\"_\")[-1].isdigit()\n",
    "    ]\n",
    "    print(len(frame_folders))\n",
    "    frame_folders.sort()\n",
    "    for frame in tqdm(frame_folders):\n",
    "        try:\n",
    "            process_frame(frame)\n",
    "        except Exception as e:\n",
    "            print(frame)\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/bean/depth/09-04-18-40-09/18_40_08_851/post.npz\"\n",
    "post = np.load(PATH)\n",
    "print(post[\"transform\"])\n",
    "\n",
    "transform_mtx = post[\"transform\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lidarpoints_to_image(\n",
    "        lidar_points: np.ndarray, width: int, height,\n",
    "        focal_length: float, cx: float, cy: float,\n",
    "    ):\n",
    "    '''\n",
    "    라이다 points 를 카메라 coordinate로 옮깁니다.\n",
    "    카메라 width, height 내부의 point만 반환합니다.\n",
    "    '''\n",
    "    transform_matrix = transform_mtx\n",
    "    lidar_points = lidar_points.reshape(-1, 3) * 1000\n",
    "\n",
    "    # 3D 라이다 포인트를 4xN 행렬로 변환\n",
    "\n",
    "    lidar_points = np.concatenate(\n",
    "        [lidar_points, np.ones((lidar_points.shape[0], 1))], axis=1\n",
    "    ).T\n",
    "\n",
    "    # 변환 행렬을 사용하여 라이다 포인트를 카메라 좌표계로 변환\n",
    "    # camera_points = transform_matrix @ lidar_points\n",
    "    camera_points = np.linalg.pinv(transform_matrix) @ lidar_points\n",
    "\n",
    "    # 카메라 좌표계의 3D 포인트를 2D 이미지 좌표로 변환\n",
    "    u = camera_points[0] * focal_length / camera_points[2] + cx\n",
    "    v = camera_points[1] * focal_length / camera_points[2] + cy\n",
    "    depth = camera_points[2]\n",
    "\n",
    "    camera_surface = np.stack([u, v, depth], axis=1)\n",
    "    lidar_points = lidar_points.T\n",
    "\n",
    "    csf = camera_surface[\n",
    "        (camera_surface[:, 0] > 0)\n",
    "        & (camera_surface[:, 0] < width)\n",
    "        & (camera_surface[:, 1] > 0)\n",
    "        & (camera_surface[:, 1] < height)\n",
    "        & (camera_surface[:, 2] > 0)\n",
    "    ]\n",
    "\n",
    "\n",
    "    csf = csf[np.argsort(csf[:, 2])[::-1]]\n",
    "    return csf\n",
    "    \n",
    "def render_2dpoint_to_image(\n",
    "        points: np.ndarray,  width: int, height: int, use_color: bool = True, MAX_DEPTH: float = 10000\n",
    "    ):\n",
    "    '''\n",
    "    projected 2d lidar points 를 하나의 이미지로 변환합니다.\n",
    "    '''\n",
    "    \n",
    "    if use_color:\n",
    "        colormap = cv2.applyColorMap(\n",
    "            np.linspace(0, 255, 256).astype(np.uint8), cv2.COLORMAP_MAGMA\n",
    "        )\n",
    "\n",
    "    canvas = (\n",
    "        np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        if use_color\n",
    "        else np.zeros((height, width), dtype=np.float32)\n",
    "    )\n",
    "    \n",
    "    if not use_color:\n",
    "        u = points[:, 0].astype(int)\n",
    "        v = points[:, 1].astype(int)\n",
    "        depth = points[:, 2]\n",
    "        u = np.clip(u, 0, width - 3)\n",
    "        v = np.clip(v, 0, height - 3)\n",
    "        \n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                canvas[v + i, u + j] = depth\n",
    "        \n",
    "        return canvas\n",
    "\n",
    "    for u, v, depth in points:\n",
    "        radius = 3\n",
    "        u = int(int(u) // 4 * 4)\n",
    "        v = int(int(v) // 4 * 4)\n",
    "        if use_color:\n",
    "            depth_color = int(np.clip(depth / MAX_DEPTH * 255, 0, 255))\n",
    "\n",
    "            # radius = int(depth / MAX_DEPTH * 10 + 5)\n",
    "            r, g, b = map(int, colormap[depth_color][0])\n",
    "            cv2.circle(canvas, (u, v), radius, (r, g, b), -1)\n",
    "        else:\n",
    "            for i in range(-radius, radius + 1):\n",
    "                for j in range(-radius, radius + 1):\n",
    "                    if (\n",
    "                        0 <= int(v) + i < height\n",
    "                        and 0 <= int(u) + j < width\n",
    "                        and np.linalg.norm([i, j]) <= radius\n",
    "                    ):\n",
    "                        canvas[int(v) + i, int(u) + j] = depth\n",
    "\n",
    "\n",
    "    colorbar = cv2.resize(colormap, (50, height))\n",
    "    return np.concatenate([canvas, colorbar], axis=1)\n",
    "\n",
    "        \n",
    "def disparity_to_depth(disparity: np.ndarray, focal_length: float, baseline: float):\n",
    "    '''\n",
    "    disparity map을 depth map으로 변환합니다\n",
    "    '''\n",
    "    disparity = disparity.astype(np.float32)\n",
    "    depth = focal_length * baseline / disparity\n",
    "    depth[depth < 0] = 0\n",
    "    depth[np.isnan(depth)] = 0\n",
    "    depth[np.isinf(depth)] = 0\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lidar_frame(lidar_points: np.ndarray, calibration: dict):\n",
    "    '''\n",
    "    라이다 포인트를 칼리브레이션을 바탕으로 처리합니다.\n",
    "    '''\n",
    "    focal_length: float = calibration[\"mtx_left\"][0,0]\n",
    "    cx: float = calibration[\"mtx_left\"][0,2]\n",
    "    cy: float = calibration[\"mtx_left\"][1,2]\n",
    "\n",
    "    image_size: Tuple[int, int] = calibration[\"image_size\"]\n",
    "    \n",
    "    lidar_camera_points = transform_lidarpoints_to_image(lidar_points, image_size[0], image_size[1], focal_length, cx, cy)\n",
    "    \n",
    "    return lidar_camera_points\n",
    "\n",
    "def process_lidar_h5file(h5file: str, overwrite: bool = False):\n",
    "    '''\n",
    "    h5file 내부의 모든 lidar points에 대해 카메라 좌표계로 변환하고, 이미지에 렌더링합니다.\n",
    "    '''\n",
    "    with h5py.File(h5file, \"a\") as f:\n",
    "        calibration = f[\"calibration\"].attrs\n",
    "        depth_median = 0\n",
    "        keys = list(f['frame'].keys())\n",
    "        if not overwrite and \"projected_points\" in f.require_group(f\"frame/{keys[-1]}\")[\"lidar\"]:\n",
    "            f.close()\n",
    "            return\n",
    "        for frame in tqdm(f[\"frame\"]):\n",
    "            \n",
    "            if not \"image_size\" in calibration:\n",
    "                image = Image.open(os.path.join(os.path.dirname(h5file), frame, \"rgb\",\"left.png\"))\n",
    "                calibration[\"image_size\"] = image.size\n",
    "            frame = f.require_group(f\"frame/{frame}\")\n",
    "            if \"projected_points\" in frame[\"lidar\"]:\n",
    "                continue\n",
    "            lidar_points = frame[\"lidar/points\"][:]\n",
    "            \n",
    "            lidar_projected_points = process_lidar_frame(lidar_points, calibration)\n",
    "            depth_median = np.max([depth_median, np.median(lidar_projected_points[:,2])])\n",
    "            if \"projected_points\" in frame[\"lidar\"]:\n",
    "                del frame[\"lidar/projected_points\"]\n",
    "            frame.create_dataset(\"lidar/projected_points\", data=lidar_projected_points)\n",
    "        \n",
    "        for frame in f[\"frame\"]:\n",
    "            if os.path.exists(os.path.join(os.path.dirname(h5file), frame, \"lidar.png\")):\n",
    "                continue\n",
    "            frame_group = f.require_group(f\"frame/{frame}\")\n",
    "            lidar_projected_points = frame_group[\"lidar/projected_points\"][:]\n",
    "            width, height = calibration[\"image_size\"]\n",
    "            rendered_image = render_2dpoint_to_image(lidar_projected_points, width,height, use_color=True, MAX_DEPTH=depth_median*5)\n",
    "            scene_folder = os.path.dirname(h5file)\n",
    "            cv2.imwrite(os.path.join(scene_folder, frame, \"lidar.png\"), rendered_image)\n",
    "            \n",
    "            # disparity_rgb = frame_group[\"disparity\"][\"rgb\"][:]\n",
    "            # focal_length = calibration[\"mtx_left\"][0,0]\n",
    "            # depth = disparity_to_depth(disparity_rgb, focal_length, np.linalg.norm(calibration[\"T\"][:]))\n",
    "            # depth = (np.clip(depth, 0, depth_median*5) / (depth_median*5) * 255.0).astype(np.uint8)\n",
    "            # depth = cv2.applyColorMap(depth, cv2.COLORMAP_MAGMA)\n",
    "            # cv2.imwrite(os.path.join(scene_folder, frame, \"depth.png\"), depth)\n",
    "        f.close()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_create_fig_png(\n",
    "    scene_folder: str,\n",
    "    frame_id: str,\n",
    "    frame: h5py.Group,\n",
    "    focal_length: float,\n",
    "    baseline: float,\n",
    "    use_numpy=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Frame에 대해 3x3 이미지 그리드를 생성합니다.\n",
    "    각 채널의 Stereo Image, Disparity, Depth 그리고 Lidar Projected Depth를 표시합니다.\n",
    "    \"\"\"\n",
    "    rgb_left = cv2.imread(os.path.join(scene_folder, frame_id, \"rgb\", \"left.png\"))\n",
    "    rgb_right = cv2.imread(os.path.join(scene_folder, frame_id, \"rgb\", \"right.png\"))\n",
    "    nir_left = cv2.imread(os.path.join(scene_folder, frame_id, \"nir\", \"left.png\"))\n",
    "    nir_right = cv2.imread(os.path.join(scene_folder, frame_id, \"nir\", \"right.png\"))\n",
    "    rgb_disparity = frame[\"disparity/rgb\"][:]\n",
    "    nir_disparity = frame[\"disparity/nir\"][:]\n",
    "\n",
    "    ########### Mis aligned frame remove\n",
    "    if rgb_disparity.mean() < 64 and nir_disparity.mean() > 64:\n",
    "        return\n",
    "\n",
    "    rgb_depth = disparity_to_depth(rgb_disparity, focal_length, baseline)\n",
    "    nir_depth = disparity_to_depth(nir_disparity, focal_length, baseline)\n",
    "    lidar_depth = frame[\"lidar/projected_points\"][:]\n",
    "\n",
    "    if use_numpy:\n",
    "        fig = frame_create_fig_png_numpy(\n",
    "            (rgb_left, rgb_right, rgb_disparity),\n",
    "            (nir_left, nir_right, nir_disparity),\n",
    "            lidar_depth,\n",
    "            focal_length, baseline\n",
    "        )\n",
    "        cv2.imwrite(os.path.join(scene_folder, frame_id, \"fig.png\"), fig)\n",
    "        return\n",
    "        # Prepare layout for 3x3 image grid\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(25, 15))\n",
    "\n",
    "    depth_max = min(\n",
    "        max(20000, max(np.median(rgb_depth), np.median(nir_depth)) * 3), 50000\n",
    "    )\n",
    "    disparity_max = min(rgb_disparity.max(), nir_disparity.max()) * 0.8\n",
    "\n",
    "    # First row: RGB images\n",
    "    axs[0, 0].imshow(cv2.cvtColor(rgb_left, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, 0].set_title(\"RGB Left\")\n",
    "    axs[0, 0].axis(\"off\")\n",
    "\n",
    "    axs[0, 1].imshow(cv2.cvtColor(rgb_right, cv2.COLOR_BGR2RGB))\n",
    "    axs[0, 1].set_title(\"RGB Right\")\n",
    "    axs[0, 1].axis(\"off\")\n",
    "\n",
    "    # RGB Disparity with magma colormap\n",
    "    im_rgb_disp = axs[0, 2].imshow(\n",
    "        np.clip(rgb_disparity, 0, disparity_max),\n",
    "        cmap=\"magma\",\n",
    "        vmin=0,\n",
    "        vmax=disparity_max,\n",
    "    )\n",
    "    axs[0, 2].set_title(\"RGB Disparity\")\n",
    "    fig.colorbar(im_rgb_disp, ax=axs[0, 2])\n",
    "    axs[0, 2].axis(\"off\")\n",
    "\n",
    "    # Second row: NIR images\n",
    "    axs[1, 0].imshow(cv2.cvtColor(nir_left, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 0].set_title(\"NIR Left\")\n",
    "    axs[1, 0].axis(\"off\")\n",
    "\n",
    "    axs[1, 1].imshow(cv2.cvtColor(nir_right, cv2.COLOR_BGR2RGB))\n",
    "    axs[1, 1].set_title(\"NIR Right\")\n",
    "    axs[1, 1].axis(\"off\")\n",
    "\n",
    "    # NIR Disparity with magma colormap\n",
    "    im_nir_disp = axs[1, 2].imshow(\n",
    "        np.clip(nir_disparity, 0, disparity_max),\n",
    "        cmap=\"magma\",\n",
    "        vmin=0,\n",
    "        vmax=disparity_max,\n",
    "    )\n",
    "    axs[1, 2].set_title(\"NIR Disparity\")\n",
    "    fig.colorbar(im_nir_disp, ax=axs[1, 2])\n",
    "    axs[1, 2].axis(\"off\")\n",
    "    rgb_depth[rgb_disparity < 1] = 0\n",
    "    nir_depth[nir_disparity < 1] = 0\n",
    "    # Third row: Depth images\n",
    "    im_rgb_depth = axs[2, 0].imshow(\n",
    "        np.clip(rgb_depth, 0, depth_max), cmap=\"magma\", vmin=0, vmax=depth_max\n",
    "    )\n",
    "    axs[2, 0].set_title(\"RGB Depth\")\n",
    "    fig.colorbar(im_rgb_depth, ax=axs[2, 0])\n",
    "    axs[2, 0].axis(\"off\")\n",
    "    im_nir_depth = axs[2, 1].imshow(\n",
    "        np.clip(nir_depth, 0, depth_max), cmap=\"magma\", vmin=0, vmax=depth_max\n",
    "    )\n",
    "    axs[2, 1].set_title(\"NIR Depth\")\n",
    "    fig.colorbar(im_nir_depth, ax=axs[2, 1])\n",
    "    axs[2, 1].axis(\"off\")\n",
    "    # Lidar depth (point cloud projected)\n",
    "    u, v = lidar_depth[:, 0], -lidar_depth[:, 1]\n",
    "    z = lidar_depth[:, 2]\n",
    "    sc = axs[2, 2].scatter(\n",
    "        u, v, c=np.clip(z, 0, depth_max), cmap=\"magma\", vmin=0, vmax=depth_max\n",
    "    )\n",
    "    axs[2, 2].set_title(\"Lidar Depth\")\n",
    "    fig.colorbar(sc, ax=axs[2, 2])\n",
    "    axs[2, 2].axis(\"off\")\n",
    "    # Display the full layout\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig(os.path.join(scene_folder, frame_id, \"fig.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def process_h5file_create_figs(h5path: str):\n",
    "    \"\"\"\n",
    "    h5file의 모든 frame에 대해 fig png 이미지들을 생성합니다.\n",
    "    \"\"\"\n",
    "    scene_id = os.path.dirname(h5path).split(\"/\")[-1]\n",
    "    with h5py.File(h5path, \"r\") as f:\n",
    "        focal_length = f[\"calibration\"].attrs[\"mtx_left\"][0, 0]\n",
    "        baseline = np.linalg.norm(f[\"calibration\"].attrs[\"T\"][:])\n",
    "        for frame_id in f[\"frame\"]:\n",
    "            frame_create_fig_png(\n",
    "                os.path.dirname(h5path),\n",
    "                frame_id,\n",
    "                f[\"frame\"].require_group(frame_id),\n",
    "                focal_length,\n",
    "                baseline,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "rgb_label = None\n",
    "nir_label = None\n",
    "colorbar_disparity = None\n",
    "\n",
    "def frame_create_fig_png_numpy(\n",
    "    rgb_tuple: Tuple[np.ndarray, np.ndarray, np.ndarray],\n",
    "    nir_tuple: Tuple[np.ndarray, np.ndarray, np.ndarray],\n",
    "    lidar_depth: np.ndarray,\n",
    "    focal_length : float, baseline: float,\n",
    "):\n",
    "    rgb_left, rgb_right, rgb_disparity = rgb_tuple\n",
    "    H,W = rgb_left.shape[:2]\n",
    "    nir_left, nir_right, nir_disparity = nir_tuple\n",
    "    rgb_disparity = rgb_disparity[:H, :W]\n",
    "    nir_disparity = nir_disparity[:H, :W]\n",
    "    #depth_max = min(max(20000, max(np.median(rgb_depth), np.median(nir_depth)) * 3), 50000)\n",
    "    disparity_max = min(rgb_disparity.max(), nir_disparity.max()) * 0.8\n",
    "    rgb_disparity_color = cv2.applyColorMap(\n",
    "        (np.clip(rgb_disparity, 0, disparity_max) / disparity_max * 255.0).astype(\n",
    "            np.uint8\n",
    "        ),\n",
    "        cv2.COLORMAP_MAGMA,\n",
    "    )\n",
    "    nir_disparity_color = cv2.applyColorMap(\n",
    "        (np.clip(nir_disparity, 0, disparity_max) / disparity_max * 255.0).astype(\n",
    "            np.uint8\n",
    "        ),\n",
    "        cv2.COLORMAP_MAGMA,\n",
    "    )\n",
    "    rgb_concat = np.concatenate([rgb_left, rgb_right, rgb_disparity_color], axis=1)\n",
    "    nir_concat = np.concatenate([nir_left, nir_right, nir_disparity_color], axis=1)\n",
    "    \n",
    "    colorbar_disparity = np.linspace(0, 255,256).reshape(256,1).astype(np.uint8)\n",
    "    colorbar_disparity = cv2.applyColorMap(colorbar_disparity, cv2.COLORMAP_MAGMA)\n",
    "    colorbar_disparity = cv2.resize(colorbar_disparity, (50, rgb_left.shape[0]))\n",
    "    colorbar_disparity_text = np.zeros((rgb_left.shape[0], 100, 3), dtype=np.uint8) + 255\n",
    "    for i in range(0, rgb_left.shape[0], 100):\n",
    "        cv2.putText(\n",
    "            colorbar_disparity_text,\n",
    "            str(int(i / rgb_left.shape[0] * disparity_max)),\n",
    "            (10, i+15),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 0, 0),\n",
    "            1,\n",
    "        )\n",
    "    colorbar_depth_text = np.zeros((rgb_left.shape[0], 100, 3), dtype=np.uint8) + 255\n",
    "    for i in range(100, rgb_left.shape[0], 100):\n",
    "        cv2.putText(\n",
    "            colorbar_depth_text,\n",
    "            str(round(focal_length * baseline / (   i / rgb_left.shape[0] * disparity_max)/1000,1)) + \"m\",\n",
    "            (10, i+15),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 0, 0),\n",
    "            1,\n",
    "        )\n",
    "    rgb_concat = np.concatenate([rgb_concat, colorbar_disparity, colorbar_disparity_text], axis=1)\n",
    "    nir_concat = np.concatenate([nir_concat, colorbar_disparity, colorbar_disparity_text], axis=1)\n",
    "    lidar_depth[:,2] = focal_length * baseline / lidar_depth[:,2]\n",
    "    lidar_disparity = render_2dpoint_to_image(lidar_depth, rgb_left.shape[1], rgb_left.shape[0], use_color=False)\n",
    "    lidar_disparity_color = cv2.applyColorMap(\n",
    "        (np.clip(lidar_disparity, 0, disparity_max) / disparity_max * 255.0).astype(np.uint8),\n",
    "        cv2.COLORMAP_MAGMA,\n",
    "    )\n",
    "    rgb_concat = np.concatenate([rgb_concat, lidar_disparity_color, colorbar_disparity, colorbar_depth_text], axis=1)\n",
    "    nir_padding = np.zeros((nir_concat.shape[0], rgb_concat.shape[1] - nir_concat.shape[1], 3), dtype=np.uint8) + 255\n",
    "    nir_concat = np.concatenate([nir_concat, nir_padding], axis=1)\n",
    "    global rgb_label, nir_label\n",
    "    if rgb_label is None:\n",
    "        rgb_label = np.zeros((100, rgb_concat.shape[1], 3), dtype=np.uint8) + 255\n",
    "        cv2.putText(rgb_label, \"RGB Left\", (W // 2, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\n",
    "        cv2.putText(rgb_label, \"RGB Right\", (W // 2 * 3, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\n",
    "        cv2.putText(rgb_label, \"RGB Disparity\", (W // 2 * 5, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\n",
    "        cv2.putText(rgb_label, \"Lidar Depth\", (W // 2 * 7, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\n",
    "        nir_label = np.zeros((100, rgb_concat.shape[1], 3), dtype=np.uint8) + 255\n",
    "        cv2.putText(nir_label, \"NIR Left\", (W // 2, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\n",
    "        cv2.putText(nir_label, \"NIR Right\", (W // 2 * 3, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\n",
    "        cv2.putText(nir_label, \"NIR Disparity\", (W // 2 * 5, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\n",
    "        \n",
    "\n",
    "    fig = np.concatenate([rgb_label, rgb_concat, nir_label, nir_concat], axis=0)\n",
    "    return fig\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from typing import Union\n",
    "from IPython.display import Video, display\n",
    "\n",
    "def save_video(image_paths: list[str], video_path, fps=5):\n",
    "    '''\n",
    "    image_paths로 읽어 온 이미지 파일들을 순서대로 비디오 파일로 저장합니다.\n",
    "    image_paths: 이미지 파일 경로 리스트\n",
    "    video_path: 저장할 비디오 파일 경로\n",
    "    fps: 초당 프레임 수\n",
    "    '''\n",
    "    print(image_paths[0])\n",
    "    height, width = cv2.imread(image_paths[0]).shape[:2]\n",
    "    # 비디오 파일 쓰기 설정\n",
    "    fourcc = cv2.VideoWriter.fourcc(*'mp4v')  # 코덱 설정\n",
    "    os.makedirs(os.path.dirname(video_path), exist_ok=True)\n",
    "    out = cv2.VideoWriter(video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for img in tqdm(image_paths):\n",
    "        img = img.replace(\"frames_cleanpass\", \"plot\")\n",
    "        if not os.path.exists(img):\n",
    "            break\n",
    "        image = cv2.imread(img)\n",
    "        out.write( image)\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"비디오가 저장되었습니다: {video_path}\")\n",
    "\n",
    "\n",
    "def process_h5_plt_video(h5file: str, create_fig=False):\n",
    "    '''\n",
    "    h5file의 모든 frame에 대해 fig.png 이미지들을 하나의 비디오로 저장합니다.\n",
    "    '''\n",
    "    with h5py.File(h5file, \"r\")  as f:\n",
    "        frame_ids = list(f[\"frame\"].keys())\n",
    "        frame_id_filtered = []\n",
    "        focal_length = f[\"calibration\"].attrs[\"mtx_left\"][0,0]\n",
    "        baseline = np.linalg.norm(f[\"calibration\"].attrs[\"T\"][:])\n",
    "        if create_fig:\n",
    "            fig_create_threads = []\n",
    "            for idx, frame_id in enumerate(tqdm(frame_ids)):\n",
    "                thread = threading.Thread(target=frame_create_fig_png, args=(os.path.dirname(h5file), frame_id, f[\"frame\"].require_group(frame_id), focal_length, baseline,  True))\n",
    "                thread.start()\n",
    "                fig_create_threads.append(thread)\n",
    "                if len(fig_create_threads) >= 6 or idx == len(frame_ids) - 1:\n",
    "                    for thread in fig_create_threads:\n",
    "                        thread.join()\n",
    "                    fig_create_threads = []        \n",
    "        for frame_id in frame_ids:\n",
    "            if not os.path.exists(os.path.join(os.path.dirname(h5file), frame_id, \"fig.png\")):\n",
    "                continue\n",
    "            if cv2.imread(os.path.join(os.path.dirname(h5file), frame_id, \"fig.png\")) is None:\n",
    "                continue\n",
    "            frame_id_filtered.append(frame_id)\n",
    "        image_paths = [os.path.join(os.path.dirname(h5file), x, \"fig.png\") for x in frame_id_filtered]\n",
    "        save_video(image_paths, h5file.replace(\".hdf5\", \".mp4\"))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stereo_depth_lidar_loss(frame: h5py.Group, focal_length: float, baseline: float):\n",
    "    disparity_rgb = frame[\"disparity/rgb\"][:]\n",
    "    disparity_nir = frame[\"disparity/nir\"][:]\n",
    "    projected_points = frame[\"lidar/projected_points\"][:]\n",
    "\n",
    "    u = projected_points[:, 0]\n",
    "    v = projected_points[:, 1]\n",
    "    z = focal_length * baseline / projected_points[:, 2]\n",
    "    depth_rgb = disparity_rgb[v.astype(int), u.astype(int)].squeeze()\n",
    "    depth_nir = disparity_nir[v.astype(int), u.astype(int)].squeeze()\n",
    "    \n",
    "    rsme_rgb = np.sqrt(np.mean((depth_rgb - z) ** 2))\n",
    "    rsme_nir = np.sqrt(np.mean((depth_nir - z) ** 2))\n",
    "    mae_rgb = np.mean(np.abs(depth_rgb - z))\n",
    "    mae_nir = np.mean(np.abs(depth_nir - z))\n",
    "    return (rsme_rgb, rsme_nir), (mae_rgb, mae_nir)\n",
    "\n",
    "\n",
    "def process_depth_loss_h5file(h5file: str, overwrite_loss=False):\n",
    "    \"\"\"\n",
    "    h5file의 모든 frame에 대해\n",
    "    raft stereo로 계산한 depth와 라이다 depth의 차이를 계산합니다.\n",
    "    rsme와 mae를 반환합니다.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5file, \"a\") as f:\n",
    "        frame_ids = list(f[\"frame\"].keys())\n",
    "        focal_length = f[\"calibration\"].attrs[\"mtx_left\"][0, 0]\n",
    "        baseline = np.linalg.norm(f[\"calibration\"].attrs[\"T\"][:])\n",
    "        __rsme_rgb = []\n",
    "        __rsme_nir = []\n",
    "        __mae_rgb = []\n",
    "        __mae_nir = []\n",
    "        if not overwrite_loss and \"rsme_rgb\" in f.attrs:\n",
    "            f.close()\n",
    "            return\n",
    "        for frame_id in tqdm(frame_ids):\n",
    "            frame = f.require_group(f\"frame/{frame_id}\")\n",
    "            if \"lidar/projected_points\" not in frame:\n",
    "                continue\n",
    "            (rsme_rgb, rsme_nir), (mae_rgb, mae_nir) = stereo_depth_lidar_loss(\n",
    "                frame, focal_length, baseline\n",
    "            )\n",
    "            frame[\"disparity/rgb\"].attrs[\"rsme\"] = rsme_rgb\n",
    "            frame[\"disparity/nir\"].attrs[\"rsme\"] = rsme_nir\n",
    "            frame[\"disparity/rgb\"].attrs[\"mae\"] = mae_rgb\n",
    "            frame[\"disparity/nir\"].attrs[\"mae\"] = mae_nir\n",
    "            __rsme_rgb.append(rsme_rgb)\n",
    "            __rsme_nir.append(rsme_nir)\n",
    "            __mae_rgb.append(mae_rgb)\n",
    "            __mae_nir.append(mae_nir)\n",
    "\n",
    "        f.attrs[\"rsme_rgb\"] = np.mean(__rsme_rgb)\n",
    "        f.attrs[\"rsme_nir\"] = np.mean(__rsme_nir)\n",
    "        f.attrs[\"mae_rgb\"] = np.mean(__mae_rgb)\n",
    "        f.attrs[\"mae_nir\"] = np.mean(__mae_nir)\n",
    "        \n",
    "        plt.plot(__rsme_rgb, label=\"RSME RGB\")\n",
    "        plt.plot(__rsme_nir, label=\"RSME NIR\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_queue(queue: list[h5py.Group], frame_ids: list[str]):\n",
    "    disparity_rgb, disparity_nir = process_frame_batch(frame_ids)\n",
    "    for idx, frame in enumerate(queue):\n",
    "        if \"disparity/rgb\" in frame:\n",
    "            del frame[\"disparity/rgb\"]\n",
    "        if \"disparity/nir\" in frame:\n",
    "            del frame[\"disparity/nir\"]\n",
    "        frame.create_dataset(\"disparity/rgb\", data=disparity_rgb[idx])\n",
    "        frame.create_dataset(\"disparity/nir\", data=disparity_nir[idx])\n",
    "        \n",
    "        if disparity_nir[idx].mean() > 64 and disparity_rgb[idx].mean() < 64:\n",
    "            frame.attrs[\"align_error\"] = True\n",
    "            print(f\"A frame {frame} has an alignment error\")\n",
    "        else:\n",
    "            frame.attrs[\"align_error\"] = False\n",
    "    queue.clear()\n",
    "    frame_ids.clear()\n",
    "\n",
    "\n",
    "def process_disparity_h5file(h5file: str, overwrite=False, batch_size = 5):\n",
    "    '''\n",
    "    h5file의 모든 frame에 대해\n",
    "    Raft Stereo 모델을 사용하여 disparity를 추출하고 h5 파일에 저장합니다.\n",
    "    disparity를 color map으로 변환하여 png 파일로 저장합니다.\n",
    "    '''\n",
    "    \n",
    "    process_queue = []\n",
    "    process_queue_ids = []\n",
    "    \n",
    "    \n",
    "    with h5py.File(h5file, \"a\")  as f:\n",
    "        frame_ids = list(f[\"frame\"].keys())\n",
    "        if not overwrite and \"disparity\" in f[\"frame\"][frame_ids[-1]]:\n",
    "            f.close()\n",
    "            return\n",
    "        for frame_id in tqdm(frame_ids):\n",
    "            frame = f.require_group(f\"frame/{frame_id}\")\n",
    "            if not overwrite and \"disparity\" in frame:\n",
    "                continue\n",
    "            process_queue.append(frame)\n",
    "            process_queue_ids.append(os.path.join(os.path.dirname(h5file), frame_id))\n",
    "            \n",
    "            if len(process_queue) >= batch_size or frame_id == frame_ids[-1]:\n",
    "                flush_queue(process_queue, process_queue_ids)\n",
    "            continue\n",
    "            output = process_frame(os.path.join(os.path.dirname(h5file), frame_id))\n",
    "            if output is None:\n",
    "                continue\n",
    "            disparity_rgb, disparity_nir = output\n",
    "            \n",
    "            frame.create_dataset(\"disparity/rgb\", data=disparity_rgb)\n",
    "            frame.create_dataset(\"disparity/nir\", data=disparity_nir)\n",
    "            if not \"disparity\" in frame:\n",
    "                continue\n",
    "            disparity_rgb = frame[\"disparity/rgb\"][:]\n",
    "            disparity_nir = frame[\"disparity/nir\"][:]\n",
    "            if disparity_nir.mean() > 64 and disparity_rgb.mean() < 64:\n",
    "                frame.attrs[\"align_error\"] = True\n",
    "                print(f\"A frame {frame_id} has an alignment error\")\n",
    "            else:\n",
    "                frame.attrs[\"align_error\"] = False\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils.hy5py import (\n",
    "    calibration_property,\n",
    "    get_frame_by_path,\n",
    "    read_calibration,\n",
    "    read_lidar,\n",
    ")\n",
    "from myutils.image_process import read_image_pair, cv2toTensor\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from myutils.points import (\n",
    "    combine_disparity_by_edge,\n",
    "    combine_disparity_by_lidar,\n",
    "    lidar_points_to_disparity,\n",
    ")\n",
    "\n",
    "frame_path = \"/bean/depth/09-08-17-27-33/17_32_56_438\"\n",
    "rgb_left, rgb_right, nir_left, nir_right = read_image_pair(frame_path)\n",
    "\n",
    "calibration = read_calibration(os.path.dirname(frame_path) + \"/0.hdf5\")\n",
    "focal_length, baseline, cx, cy = calibration_property(calibration)\n",
    "transform_mtx = np.load(\"jai_transform.npy\")\n",
    "with get_frame_by_path(frame_path) as frame:\n",
    "    lidar_disparity = lidar_points_to_disparity(\n",
    "        frame[\"lidar/points\"][:] * 1000,\n",
    "        transform_mtx,\n",
    "        focal_length,\n",
    "        baseline,\n",
    "        cx,\n",
    "        cy,\n",
    "    )\n",
    "    flow_rgb: np.ndarray = frame[\"disparity/rgb\"][:] # type: ignore\n",
    "    flow_nir: np.ndarray = frame[\"disparity/nir\"][:] # type: ignore\n",
    "    depth_mono_rgb: np.ndarray = frame[\"depth_mono/rgb\"][:] # type: ignore\n",
    "    depth_mono_nir: np.ndarray = frame[\"depth_mono/nir\"][:] # type: ignore\n",
    "\n",
    "disparity_combined = combine_disparity_by_lidar(\n",
    "    lidar_disparity, flow_rgb, flow_nir, 24, 24\n",
    ")\n",
    "disparity_combined_2 = combine_disparity_by_edge(\n",
    "    lidar_disparity, flow_rgb, flow_nir, rgb_left, nir_left, 24, 24\n",
    ")\n",
    "\n",
    "print(disparity_combined.mean(), disparity_combined.max(), disparity_combined.shape, disparity_combined.dtype)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(241)\n",
    "plt.imshow(rgb_left)\n",
    "plt.subplot(242)\n",
    "plt.imshow(flow_rgb, cmap=\"magma\", vmin=0, vmax=32)\n",
    "plt.subplot(243)\n",
    "plt.imshow(depth_mono_rgb, cmap=\"magma\", vmin=0, vmax=32)\n",
    "plt.subplot(244)\n",
    "plt.imshow(disparity_combined, cmap=\"magma\", vmin=0, vmax=32)\n",
    "plt.subplot(245)\n",
    "plt.imshow(nir_left, cmap=\"gray\")\n",
    "plt.subplot(246)\n",
    "plt.imshow(flow_nir, cmap=\"magma\", vmin=0, vmax=32)\n",
    "plt.subplot(247)\n",
    "plt.imshow(depth_mono_nir, cmap=\"magma\", vmin=0, vmax=32)\n",
    "plt.subplot(248)\n",
    "plt.imshow(disparity_combined_2, cmap=\"magma\", vmin=0, vmax=32)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_h5file_all(\n",
    "    h5file: str,\n",
    "    overwrite_disparity=False,\n",
    "    overwrite_loss=False,\n",
    "    overwrite_plot=False,\n",
    "    overwrite_lidar=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    h5file에 대해 disparity, plot, lidar를 모두 처리합니다.\n",
    "    \"\"\"\n",
    "    process_disparity_h5file(h5file, overwrite=overwrite_disparity)\n",
    "    process_lidar_h5file(h5file, overwrite=overwrite_lidar)\n",
    "    process_depth_loss_h5file(h5file, overwrite_loss=overwrite_loss)\n",
    "    if overwrite_plot or not os.path.exists(h5file.replace(\".hdf5\", \".mp4\")):\n",
    "        process_h5_plt_video(h5file, create_fig=True)\n",
    "\n",
    "    print(f\"Finished processing {h5file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_disparity_h5file(\"/bean/depth/09-10-14-59-19/0.hdf5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [x for x in os.listdir(\"/bean/depth\") if x.startswith(\"09-20\")]\n",
    "for folder in folders:\n",
    "    print(f\"Processing {folder}\")\n",
    "    h5files = [os.path.join(\"/bean/depth\", folder, x) for x in os.listdir(os.path.join(\"/bean/depth\", folder)) if x.endswith(\".hdf5\")]\n",
    "    h5files.sort(key=lambda x: int(os.path.basename(x).split(\".\")[0]))\n",
    "    for h5file in h5files:\n",
    "        print(f\"Processing {h5file}\")\n",
    "        process_h5file_all(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FOLDERS = [x for x in os.listdir(\"/bean/depth\") if os.path.isdir( os.path.join(\"/bean/depth\",x))]\n",
    "FOLDERS = [x for x in FOLDERS if x.startswith(\"09-20\")]\n",
    "FOLDERS.sort()\n",
    "h5files_pending = []\n",
    "for folder in FOLDERS:\n",
    "    print(folder)\n",
    "    h5files = os.listdir(os.path.join(\"/bean/depth\",folder))\n",
    "    h5files = [x for x in h5files if x.endswith(\".hdf5\")]\n",
    "    h5files.sort(key=lambda x : int(x.split('.')[0]) )\n",
    "    for h5 in h5files:\n",
    "        h5files_pending.append(os.path.join(\"/bean/depth\",folder,h5))\n",
    "print(len(h5files_pending))\n",
    "for h5 in tqdm(h5files_pending):\n",
    "    process_disparity_h5file(h5,overwrite=True, batch_size=5)\n",
    "    #process_h5file_all(h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"CostDCNet\")\n",
    "\n",
    "\n",
    "from task import Sub_trainer\n",
    "from options import Options\n",
    "os.chdir(\"CostDCNet\")\n",
    "opts = Options().parse()\n",
    "trainer = Sub_trainer(opts)\n",
    "trainer.evaluate(is_offline=True)\n",
    "os.chdir(\"..\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils.depth import get_depth_anything_model\n",
    "from typing import Union\n",
    "depth_anything = get_depth_anything_model()\n",
    "import numpy as np\n",
    "from myutils.image_process import cv2toTensor\n",
    "import torch\n",
    "\n",
    "def inference_depth(image: Union[np.ndarray, torch.Tensor]):\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = cv2toTensor(image).cuda()\n",
    "    depth = depth_anything(image)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from myutils.hy5py import calibration_property, read_calibration\n",
    "from myutils.points import (\n",
    "    combine_disparity_by_lidar,\n",
    "    project_points_on_camera,\n",
    "    transform_point_inverse,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform_mtx = np.load(\"jai_transform.npy\")\n",
    "\n",
    "\n",
    "def process_frame_disparity_refine(frame: h5py.Group, h5file: str, calibration):\n",
    "    scene_folder = os.path.dirname(h5file)\n",
    "    image_left_path = os.path.join(scene_folder, frame[\"image\"].attrs[\"rgb_left_path\"])\n",
    "    image_left_nir_path = os.path.join(\n",
    "        scene_folder, frame[\"image\"].attrs[\"nir_left_path\"]\n",
    "    )\n",
    "    image_left = (\n",
    "        cv2toTensor(cv2.imread(image_left_path, cv2.IMREAD_COLOR)).cuda() / 255.0\n",
    "    )\n",
    "    image_left_nir = (\n",
    "        cv2toTensor(cv2.imread(image_left_nir_path, cv2.IMREAD_GRAYSCALE))\n",
    "        .cuda()\n",
    "        .repeat(1, 3, 1, 1)\n",
    "        / 255.0\n",
    "    )\n",
    "    image_left = torch.cat([image_left, image_left_nir], dim=0)\n",
    "    depth = inference_depth(image_left)\n",
    "    depth = depth.cpu().numpy().squeeze()\n",
    "    depth_rgb, depth_nir = depth\n",
    "    if \"depth_mono/rgb\" in frame:\n",
    "        del frame[\"depth_mono/rgb\"]\n",
    "    if \"depth_mono/nir\" in frame:\n",
    "        del frame[\"depth_mono/nir\"]\n",
    "    frame.create_dataset(\"depth_mono/rgb\", data=depth_rgb)\n",
    "    frame.create_dataset(\"depth_mono/nir\", data=depth_nir)\n",
    "\n",
    "\n",
    "def process_h5file_disparity_refine(h5file: str):\n",
    "    with h5py.File(h5file, \"a\") as f:\n",
    "        calibration = calibration_property(read_calibration(f))\n",
    "        for frame_id in tqdm(f[\"frame\"]):\n",
    "            frame = f[\"frame\"][frame_id]\n",
    "            process_frame_disparity_refine(frame, h5file, calibration)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import h5py\n",
    "import torch\n",
    "import os\n",
    "from myutils.depth import get_depth_anything_model\n",
    "from myutils.image_process import cv2toTensor\n",
    "from myutils.points import pad_lidar_points, refine_disparity_with_monodepth, transform_point_inverse, project_points_on_camera, combine_disparity_by_lidar\n",
    "from tqdm.notebook import tqdm\n",
    "from myutils.hy5py import calibration_property, read_calibration\n",
    "depth_anything = get_depth_anything_model()\n",
    "transform_mtx = np.load(\"jai_transform.npy\")\n",
    "\n",
    "print(transform_mtx)\n",
    "def process_h5file_disparity_merge(h5file: str, at=0):\n",
    "    with h5py.File(h5file, \"a\") as f:\n",
    "        calibration = calibration_property(read_calibration(f))\n",
    "        fx, bs, cx, cy = calibration\n",
    "        idx = 0\n",
    "        for frame_id in tqdm(f[\"frame\"]):\n",
    "            if idx < at:\n",
    "                idx += 1\n",
    "                continue\n",
    "            frame = f[\"frame\"][frame_id]\n",
    "            disparity_rgb = frame[\"disparity/rgb\"][:]\n",
    "            disparity_nir = frame[\"disparity/nir\"][:]\n",
    "            image_left = cv2.imread(os.path.join(os.path.dirname(h5file), frame_id, \"rgb\", \"left.png\"))\n",
    "            monodepth_rgb = frame[\"depth_mono/rgb\"][:]\n",
    "            monodepth_nir = frame[\"depth_mono/nir\"][:]\n",
    "            #monodepth_rgb = depth_anything(cv2toTensor(image_left).cuda()).cpu()[0].numpy()\n",
    "            image_left_nir = cv2.imread(os.path.join(os.path.dirname(h5file), frame_id, \"nir\", \"left.png\"))\n",
    "            #monodepth_nir = depth_anything(cv2toTensor(image_left_nir).cuda()).cpu()[0].numpy()\n",
    "            disparity_rgb = refine_disparity_with_monodepth(\n",
    "                disparity_rgb, monodepth_rgb\n",
    "            )\n",
    "            disparity_nir = refine_disparity_with_monodepth(\n",
    "                disparity_nir, monodepth_nir\n",
    "            )\n",
    "            lidar_points = frame[\"lidar/points\"][:] * 1000\n",
    "            lidar_points = transform_point_inverse(lidar_points, transform_mtx)\n",
    "            lidar_points = project_points_on_camera(lidar_points, fx, cx, cy, 720,540)\n",
    "            lidar_points[:,2 ] = bs * fx / lidar_points[:,2] - 1\n",
    "            lidar_points = pad_lidar_points(lidar_points, 5000)\n",
    "            \n",
    "            disparity = combine_disparity_by_lidar(lidar_points, disparity_rgb, disparity_nir, 48, 48)\n",
    "            \n",
    "            break\n",
    "        f.close()\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(disparity_rgb, cmap=\"magma\", vmin = 0, vmax = 16)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(disparity_nir, cmap=\"magma\", vmin = 0, vmax = 16)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(disparity, cmap=\"magma\", vmin = 0, vmax = 16)\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(monodepth_rgb, cmap=\"magma\", vmin = 0, vmax = 16)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(monodepth_nir, cmap=\"magma\", vmin = 0, vmax = 16)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 가우시안 잡음의 표준 편차\n",
    "sigma = 1.0\n",
    "\n",
    "# 윈도우 크기 (현재 예시에서는 각 픽셀을 독립적으로 처리)\n",
    "window_size = 5\n",
    "\n",
    "def initialize(A, B):\n",
    "    F = (A + B) / 2\n",
    "    alpha = np.zeros(A.shape + (2,), dtype=int)  # 마지막 차원: A, B\n",
    "    beta = np.zeros(A.shape + (2,))\n",
    "    return F, alpha, beta\n",
    "\n",
    "def e_step(A, B, F, beta, sigma):\n",
    "    # \\alpha의 가능한 값: 1, 0, -1\n",
    "    alphas = np.array([1, 0, -1])\n",
    "    \n",
    "    # Initialize responsibility arrays\n",
    "    gamma_A = np.zeros(A.shape + (len(alphas),))\n",
    "    gamma_B = np.zeros(B.shape + (len(alphas),))\n",
    "    \n",
    "    # Compute likelihoods for each possible alpha\n",
    "    for idx, alpha_val in enumerate(alphas):\n",
    "        # For image A\n",
    "        L_A = (1 / np.sqrt(2 * np.pi * sigma**2)) * \\\n",
    "              np.exp(-((A - alpha_val * F - beta[..., 0])**2) / (2 * sigma**2))\n",
    "        gamma_A[..., idx] = L_A\n",
    "        \n",
    "        # For image B\n",
    "        L_B = (1 / np.sqrt(2 * np.pi * sigma**2)) * \\\n",
    "              np.exp(-((B - alpha_val * F - beta[..., 1])**2) / (2 * sigma**2))\n",
    "        gamma_B[..., idx] = L_B\n",
    "    \n",
    "    # Sum over all possible alphas for normalization\n",
    "    sum_gamma = gamma_A.sum(axis=-1) + gamma_B.sum(axis=-1) + 1e-8  # Avoid division by zero\n",
    "    \n",
    "    # Compute posterior probabilities\n",
    "    gamma_A /= sum_gamma[..., np.newaxis]\n",
    "    gamma_B /= sum_gamma[..., np.newaxis]\n",
    "    \n",
    "    return gamma_A, gamma_B\n",
    "\n",
    "def m_step(A, B, gamma_A, gamma_B, alphas):\n",
    "    # alphas: (3,) -> reshape to (1, 1, 3) for broadcasting with (540,720,3)\n",
    "    alphas_reshaped = alphas[np.newaxis, np.newaxis, :]  # (1, 1, 3)\n",
    "    \n",
    "    # 확장된 A와 B\n",
    "    A_expanded = A[..., np.newaxis]  # (540, 720, 1)\n",
    "    B_expanded = B[..., np.newaxis]  # (540, 720, 1)\n",
    "    \n",
    "    # Update F\n",
    "    numerator = (gamma_A * alphas_reshaped * A_expanded +\n",
    "                 gamma_B * alphas_reshaped * B_expanded)\n",
    "    denominator = (gamma_A + gamma_B) * alphas_reshaped + 1e-8  # Avoid division by zero\n",
    "    \n",
    "    # Compute F as weighted average\n",
    "    F_new = numerator.sum(axis=-1) / denominator.sum(axis=-1)\n",
    "    \n",
    "    # Update beta\n",
    "    beta_A = (A_expanded - F_new[..., np.newaxis] * alphas_reshaped) * gamma_A\n",
    "    beta_B = (B_expanded - F_new[..., np.newaxis] * alphas_reshaped) * gamma_B\n",
    "    \n",
    "    # Sum over alphas\n",
    "    beta_A = beta_A.sum(axis=-1) / (gamma_A.sum(axis=-1) + 1e-8)\n",
    "    beta_B = beta_B.sum(axis=-1) / (gamma_B.sum(axis=-1) + 1e-8)\n",
    "    \n",
    "    # Update alpha by selecting the alpha with the highest posterior probability\n",
    "    alpha_A = alphas[np.argmax(gamma_A, axis=-1)]\n",
    "    alpha_B = alphas[np.argmax(gamma_B, axis=-1)]\n",
    "    \n",
    "    return F_new, alpha_A, alpha_B, beta_A, beta_B\n",
    "\n",
    "def em_algorithm(A, B, max_iters=100, tol=1e-3):\n",
    "    F, alpha, beta = initialize(A, B)\n",
    "    alphas = np.array([1, 0, -1])\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        F_old = F.copy()\n",
    "        \n",
    "        # E 단계\n",
    "        gamma_A, gamma_B = e_step(A, B, F, beta, sigma)\n",
    "        \n",
    "        # M 단계\n",
    "        F, alpha_A, alpha_B, beta_A, beta_B = m_step(A, B, gamma_A, gamma_B, alphas)\n",
    "        beta = np.stack([beta_A, beta_B], axis=-1)\n",
    "        \n",
    "        # 알파 업데이트\n",
    "        alpha[..., 0] = alpha_A\n",
    "        alpha[..., 1] = alpha_B\n",
    "        \n",
    "        # 수렴 검사\n",
    "        delta = np.linalg.norm(F - F_old)\n",
    "        print(f\"Iteration {iteration+1}, delta={delta:.6f}\")\n",
    "        if delta < tol:\n",
    "            print(\"수렴 완료.\")\n",
    "            break\n",
    "    return F, alpha, beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "def calculate_cdf(hist):\n",
    "    cdf = hist.cumsum()\n",
    "    cdf_normalized = cdf / cdf[-1]\n",
    "    return cdf_normalized\n",
    "\n",
    "def estimate_exposure_ratio(gray1, gray2):\n",
    "    # 평균 밝기 계산\n",
    "    mean1 = np.mean(gray1)\n",
    "    mean2 = np.mean(gray2)\n",
    "    \n",
    "\n",
    "    exposure_ratio = mean2 / mean1\n",
    "    \n",
    "    \n",
    "    # 비율을 로그 값으로 변환 (HDR 알고리즘에서 사용)\n",
    "    \n",
    "    log_exposure_ratio = np.log2(exposure_ratio)\n",
    "    \n",
    "    return log_exposure_ratio\n",
    "def compute_hdr(img1, img2):\n",
    "    # if img1.mean() > img2.mean():\n",
    "    #     img1, img2 = img2, img1\n",
    "    print(\"exposure time\")\n",
    "    exposure_ratio = estimate_exposure_ratio(img1, img2)\n",
    "    print(exposure_ratio)\n",
    "    exposure_times = np.array([1.0,2.0**exposure_ratio], dtype=np.float32)\n",
    "    img1 = np.repeat(img1[..., np.newaxis] * 255, 3, axis=-1).astype(np.uint8)\n",
    "    img2 = np.repeat(img2[..., np.newaxis] * 255, 3, axis=-1).astype(np.uint8)\n",
    "    print(img1.shape, img2.shape)\n",
    "    images = [img1, img2]\n",
    "    print(exposure_times)\n",
    "    merge_debevec =  cv2.createMergeRobertson()\n",
    "    #exposure_times=[1.0, 2.0]\n",
    "    hdr = merge_debevec.process(images, times=exposure_times)\n",
    "    print(\"hdr\")\n",
    "    return hdr\n",
    "\n",
    "def local_histogram_matching(source_gray, template_gray, clip_limit=2.0, tile_grid_size=(24,24), exposure_limit=0.1):\n",
    "    source_gray = (source_gray * 255).astype(np.uint8)\n",
    "    template_gray = (template_gray * 255).astype(np.uint8)\n",
    "    # CLAHE 적용하여 대비 향상\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    source_clahe = clahe.apply(source_gray)\n",
    "    template_clahe = clahe.apply(template_gray)\n",
    "    \n",
    "    # 히스토그램 매칭 수행\n",
    "    matched = exposure.match_histograms(source_clahe, template_clahe)\n",
    "    matched = cv2.fastNlMeansDenoising(matched.astype(np.uint8), None, h=10, templateWindowSize=15, searchWindowSize=21)\n",
    "    matched[template_gray < exposure_limit * 255] = source_gray[template_gray < exposure_limit * 255]\n",
    "    return matched / 255\n",
    "\n",
    "def false_fusion_algorithm_2(rgb: np.ndarray, nir: np.ndarray):\n",
    "    yuv_mat = np.array([[0.299, 0.587, 0.114], [-0.147, -0.289, 0.436], [0.615, -0.515, -0.100]])\n",
    "    yuv = np.dot(rgb, yuv_mat.T)\n",
    "    y, u, v = yuv[:,:,0], yuv[:,:,1], yuv[:,:,2]\n",
    "    yh = y\n",
    "    yh = compute_hdr(y,nir).mean(axis=-1)\n",
    "    yh2 = nir\n",
    "    # yh2 = (yh + y) / 2\n",
    "    # yh = (yh + nir) / 2\n",
    "    \n",
    "    #yh = local_histogram_matching(y, nir)\n",
    "    u = u * yh / y\n",
    "\n",
    "    v = v * yh / y\n",
    "    y_old = y\n",
    "    \n",
    "    print(yh.shape, nir.shape)\n",
    "    #yn = yh\n",
    "    mul = 25\n",
    "    y_em, alpha, beta = em_algorithm(yh * mul , yh2 * mul, max_iters = 25)\n",
    "    y_em /= mul\n",
    "    \n",
    "    #y_em = yh\n",
    "    print(yh.max())\n",
    "    \n",
    "    print(\"eval y\", yh.mean(), y.mean(), y_em.mean(), nir.mean())\n",
    "    un = u  - nir\n",
    "    vn = nir - v\n",
    "    y = yh\n",
    "    \n",
    "    yn = np.sqrt(np.var(y)) / np.sqrt(np.var(y_em)) * (y_em - np.mean(y_em)) + np.mean(y)\n",
    "    un = np.sqrt(np.var(u)) / np.sqrt(np.var(un)) * (un - np.mean(un)) + np.mean(u)\n",
    "    \n",
    "    d = np.abs(nir - np.mean(nir))\n",
    "    mu = d / np.mean(d)\n",
    "    vn = mu * np.sqrt(np.var(v)) / np.sqrt(np.var(vn)) * (vn - np.mean(vn)) + np.mean(v)\n",
    "    \n",
    "    yuv = np.stack([yn,un,vn], axis=2)\n",
    "    rgb_mat = np.array([[1, 0, 1.13983], [1, -0.39465, -0.58060], [1, 2.03211, 0]])\n",
    "    rgb = np.dot(yuv, rgb_mat.T)\n",
    "    return rgb, [np.dot(np.stack([y_em,un, vn], axis=2), rgb_mat.T), yh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from myutils.image_process import cv2toTensor\n",
    "from myutils.color_fusion import false_fusion_algorithm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "frame_folder = \"/bean/depth/09-28-21-15-50/21_31_01_725\"\n",
    "#frame_folder = \"/bean/depth/09-08-17-27-33/17_35_36_287\"\n",
    "\n",
    "#frame_folder = \"/bean/depth/09-28-17-34-59/17_36_28_144\"\n",
    "#frame_folder = \"/bean/depth/09-28-17-34-59/17_35_46_519\"\n",
    "#frame_folder = \"/bean/depth/09-28-21-15-50/21_34_03_125\"\n",
    "#frame_folder = \"/bean/depth/09-08-17-27-33/17_33_16_354\"\n",
    "rgb_left = cv2.imread(os.path.join(frame_folder, \"rgb\", \"left.png\"))\n",
    "rgb_left = cv2.cvtColor(rgb_left, cv2.COLOR_BGR2RGB).astype(np.float32) / 255\n",
    "nir_left = cv2.imread(os.path.join(frame_folder, \"nir\", \"left.png\"), cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255\n",
    "fusion, [y_h, y_em] = false_fusion_algorithm_2(rgb_left, nir_left)\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(131)\n",
    "sc = plt.imshow(y_h, vmin=0, vmax=1)\n",
    "plt.subplot(132)\n",
    "sc = plt.imshow(y_em, vmin=0, vmax=1)\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(131)\n",
    "plt.imshow(rgb_left)\n",
    "plt.subplot(132)\n",
    "plt.imshow(nir_left, cmap=\"gray\")\n",
    "plt.subplot(133)\n",
    "plt.imshow(fusion)\n",
    "plt.show()\n",
    "rgb_right = cv2.imread(os.path.join(frame_folder, \"rgb\", \"right.png\"))\n",
    "nir_right = cv2.imread(os.path.join(frame_folder, \"nir\", \"right.png\"), cv2.IMREAD_GRAYSCALE) / 255\n",
    "rgb_right = cv2.cvtColor(rgb_right, cv2.COLOR_BGR2RGB).astype(np.float32) / 255\n",
    "\n",
    "fusion_right, _ = false_fusion_algorithm_2(rgb_right, nir_right)\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(131)\n",
    "plt.imshow(rgb_right)\n",
    "plt.subplot(132)\n",
    "plt.imshow(nir_right, cmap=\"gray\")\n",
    "plt.subplot(133)\n",
    "plt.imshow(fusion_right)\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, flow = model(cv2toTensor(rgb_left * 255).cuda(), cv2toTensor(rgb_right * 255).cuda(), test_mode = True)\n",
    "    flow = -flow.cpu()[0].numpy().squeeze()\n",
    "    _, flow_nir = model(cv2toTensor(nir_left * 255).cuda().repeat(1,3,1,1), cv2toTensor(nir_right * 255).cuda().repeat(1,3,1,1), test_mode = True)\n",
    "    flow_nir = -flow_nir.cpu()[0].numpy().squeeze()\n",
    "    _, flow_fusion = model(cv2toTensor(fusion * 255).cuda(), cv2toTensor(fusion_right * 255).cuda(), test_mode = True)\n",
    "    flow_fusion = -flow_fusion.cpu()[0].numpy().squeeze()\n",
    "print(nir_right.min(), nir_right.max())\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplot(131)\n",
    "plt.imshow(flow, cmap=\"magma\", vmin=0, vmax=32)\n",
    "plt.subplot(132)\n",
    "\n",
    "plt.imshow(flow_nir, cmap=\"magma\", vmin=0, vmax=32)\n",
    "plt.subplot(133)\n",
    "plt.imshow(flow_fusion, cmap=\"magma\", vmin=0, vmax=32)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_h5file_disparity_merge(\"/bean/depth/09-05-17-07-26/0.hdf5\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "depth = inference_depth(cv2.imread(\"/bean/depth/09-28-17-06-04/17_10_22_823/nir/left.png\"))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(depth.cpu().numpy()[0], cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from myutils.hy5py import read_calibration\n",
    "\n",
    "with h5py.File(\"/bean/depth/09-09-19-46-45/0.hdf5\", \"r\") as f:\n",
    "    calibration = f[\"calibration\"].attrs\n",
    "    print(calibration[\"mtx_left\"])\n",
    "    print(calibration[\"mtx_right\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2310 - 2160 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"/bean/depth/09-09-19-46-45\"\n",
    "h5files = os.listdir(FOLDER)\n",
    "h5files = [x for x in h5files if x.endswith(\".hdf5\")]\n",
    "FRAME_COUNT = 25\n",
    "__rsme_rgb = []\n",
    "__rsme_nir = []\n",
    "__mae_rgb = []\n",
    "__mae_nir = []\n",
    "__rsme_rgb_per_frame = []\n",
    "__rsme_nir_per_frame = []\n",
    "__mae_rgb_per_frame = []\n",
    "__mae_nir_per_frame = []\n",
    "for h5file in tqdm(h5files):\n",
    "    h5file = os.path.join(FOLDER, h5file)\n",
    "    \n",
    "    with h5py.File(h5file, \"r\") as f:\n",
    "        # print(f.attrs[\"rsme_rgb\"], f.attrs[\"rsme_nir\"])\n",
    "        # print(f.attrs[\"mae_rgb\"], f.attrs[\"mae_nir\"])\n",
    "        # __rsme_rgb.append(f.attrs[\"rsme_rgb\"])\n",
    "        # __rsme_nir.append(f.attrs[\"rsme_nir\"])\n",
    "        # __mae_rgb.append(f.attrs[\"mae_rgb\"])\n",
    "        # __mae_nir.append(f.attrs[\"mae_nir\"])\n",
    "        for frame in f[\"frame\"]:\n",
    "            frame = f.require_group(f\"frame/{frame}\")\n",
    "            __rsme_rgb_per_frame.append(frame[\"disparity/rgb\"].attrs[\"rsme\"])\n",
    "            __rsme_nir_per_frame.append(frame[\"disparity/nir\"].attrs[\"rsme\"])\n",
    "            __mae_rgb_per_frame.append(frame[\"disparity/rgb\"].attrs[\"mae\"])\n",
    "            __mae_nir_per_frame.append(frame[\"disparity/nir\"].attrs[\"mae\"])\n",
    "            if len(__rsme_rgb_per_frame) > FRAME_COUNT:\n",
    "                __rsme_rgb.append(np.mean(__rsme_rgb_per_frame))\n",
    "                __rsme_nir.append(np.mean(__rsme_nir_per_frame))\n",
    "                __mae_rgb.append(np.mean(__mae_rgb_per_frame))\n",
    "                __mae_nir.append(np.mean(__mae_nir_per_frame))\n",
    "                __rsme_rgb_per_frame = []\n",
    "                __rsme_nir_per_frame = []\n",
    "                __mae_rgb_per_frame = []\n",
    "                __mae_nir_per_frame = []\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(__rsme_rgb)) * FRAME_COUNT, __rsme_rgb, label=\"RGB\")\n",
    "plt.plot(np.arange(len(__rsme_nir)) * FRAME_COUNT, __rsme_nir, label=\"NIR\")\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(len(__mae_rgb)) * FRAME_COUNT, __mae_rgb, label=\"RGB\")\n",
    "plt.plot(np.arange(len(__mae_nir)) * FRAME_COUNT, __mae_nir, label=\"NIR\")\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.plot(__rsme_rgb_per_frame, label=\"RSME RGB per frame\")\n",
    "# plt.plot(__rsme_nir_per_frame, label=\"RSME NIR per frame\")\n",
    "# plt.xlabel(\"Frame\")\n",
    "# plt.ylabel(\"RSME\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.plot(__mae_rgb_per_frame, label=\"MAE RGB per frame\")\n",
    "# plt.plot(__mae_nir_per_frame, label=\"MAE NIR per frame\")\n",
    "# plt.xlabel(\"Frame\")\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(__rsme_rgb, label=\"RSME RGB\")\n",
    "plt.plot(__rsme_nir, label=\"RSME NIR\")\n",
    "plt.xlabel(\"Scene\")\n",
    "plt.ylabel(\"RSME\")\n",
    "plt.ylim(0, 10)  # Set the y-axis range\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(__mae_rgb, label=\"MAE RGB\")\n",
    "plt.plot(__mae_nir, label=\"MAE NIR\")\n",
    "plt.xlabel(\"Scene\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.ylim(0, 5)  # Set the y-axis range\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(__rsme_rgb_per_frame, label=\"RSME RGB per frame\")\n",
    "plt.plot(__rsme_nir_per_frame, label=\"RSME NIR per frame\")\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"RSME\")\n",
    "plt.ylim(0, 10)  # Set the y-axis range\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(__mae_rgb_per_frame, label=\"MAE RGB per frame\")\n",
    "plt.plot(__mae_nir_per_frame, label=\"MAE NIR per frame\")\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.ylim(0, 5)  # Set the y-axis range\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "def evaluate_exposure(image):\n",
    "    if image.mean() < 12:\n",
    "        return -1\n",
    "    if image.mean() > 196:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def evaluate_exposure_stereo(image_left, image_right):\n",
    "    left_intensity = image_left.mean()\n",
    "    right_intensity = image_right.mean()\n",
    "    if left_intensity / right_intensity > 1.15:\n",
    "        return 1\n",
    "    if right_intensity / left_intensity > 1.15:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "def fft_blur_detection(image):\n",
    "    \"\"\"\n",
    "    Detect blur in an image using FFT (Fast Fourier Transform).\n",
    "    Steps:\n",
    "    1. Convert image to grayscale.\n",
    "    2. Apply Fourier Transform.\n",
    "    3. Analyze high frequency components to determine blur level.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input BGR image\n",
    "    \n",
    "    Returns:\n",
    "    - blur_metric: A scalar representing the amount of blur in the image.\n",
    "                   Lower values indicate more blur.\n",
    "    \"\"\"\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
    "    \n",
    "    # Get the image dimensions\n",
    "    (h, w) = gray.shape\n",
    "    \n",
    "    # Apply a Fast Fourier Transform (FFT)\n",
    "    f_transform = np.fft.fft2(gray)\n",
    "    f_shifted = np.fft.fftshift(f_transform)\n",
    "    \n",
    "    # Compute the magnitude spectrum\n",
    "    magnitude_spectrum = 20 * np.log(np.abs(f_shifted))\n",
    "    \n",
    "    # Crop the magnitude spectrum to remove the low frequencies\n",
    "    # Center coordinates\n",
    "    center_row, center_col = h // 2, w // 2\n",
    "    # Radius of the low frequency region to remove\n",
    "    radius = 60\n",
    "    \n",
    "    # Create a mask to zero out the low frequencies\n",
    "    mask = np.ones((h, w), np.uint8)\n",
    "    cv2.circle(mask, (center_col, center_row), radius, 0, -1)\n",
    "    \n",
    "    # Apply the mask to the magnitude spectrum\n",
    "    f_shifted_masked = f_shifted * mask\n",
    "    \n",
    "    # Compute the inverse FFT\n",
    "    f_ishift = np.fft.ifftshift(f_shifted_masked)\n",
    "    reconstructed_image = np.fft.ifft2(f_ishift)\n",
    "    reconstructed_image = np.abs(reconstructed_image)\n",
    "    \n",
    "    # Compute the variance of the reconstructed image (sharpness measure)\n",
    "    blur_metric = np.var(reconstructed_image)\n",
    "    \n",
    "    return blur_metric\n",
    "\n",
    "\n",
    "def lidar_box_error(lidar_points, disparity_map, box_size=5):\n",
    "    \"\"\"\n",
    "    벡터화된 방식으로 LiDAR 포인트와 Disparity Map을 이용하여 Box Error를 계산합니다.\n",
    "\n",
    "    Parameters:\n",
    "    - lidar_points: (N, 3) 배열로 각 행이 (u, v, d)를 나타냅니다.\n",
    "    - disparity_map: 2D 배열로 Disparity Map을 나타냅니다.\n",
    "    - box_size: 박스의 크기 (기본값은 5).\n",
    "\n",
    "    Returns:\n",
    "    - mean_box_error: 모든 포인트에 대한 평균 Box Error.\n",
    "    - errors: 각 포인트에 대한 [u, v, box_error] 배열.\n",
    "    \"\"\"\n",
    "    # LiDAR 포인트 분리\n",
    "    u, v, d = lidar_points.T\n",
    "    h, w = disparity_map.shape\n",
    "    half_box = box_size // 2\n",
    "\n",
    "    # Disparity Map을 패딩하여 경계 문제를 해결\n",
    "    padded_disparity = np.pad(disparity_map, \n",
    "                              pad_width=((half_box, half_box), (half_box, half_box)), \n",
    "                              mode='edge')\n",
    "\n",
    "    # 슬라이딩 윈도우을 이용해 모든 5x5 박스를 생성\n",
    "    windows = sliding_window_view(padded_disparity, (box_size, box_size))\n",
    "    # windows의 shape: (h, w, box_size, box_size)\n",
    "\n",
    "    # u와 v가 이미지 경계를 벗어나지 않도록 클리핑\n",
    "    u_clipped = np.clip(u, 0, w - 1).astype(int)\n",
    "    v_clipped = np.clip(v, 0, h - 1).astype(int)\n",
    "\n",
    "    # 각 포인트에 해당하는 박스 추출\n",
    "    # indexing을 통해 (N, box_size, box_size) 형태의 배열을 얻음\n",
    "    boxes = windows[v_clipped, u_clipped, :, :]\n",
    "\n",
    "    # 실제 d 값과 박스 내 값들의 절대 차이 계산\n",
    "    diffs = np.abs(boxes - d[:, np.newaxis, np.newaxis])\n",
    "\n",
    "    # 박스 에러 (절대 차이의 평균) 계산\n",
    "    box_errors = np.mean(diffs, axis=(1, 2))\n",
    "\n",
    "    # NaN 값이 있는 경우 제거\n",
    "    valid = ~np.isnan(box_errors)\n",
    "    box_errors = box_errors[valid]\n",
    "    u_valid = u_clipped[valid]\n",
    "    v_valid = v_clipped[valid]\n",
    "\n",
    "    # 전체 평균 박스 에러 계산\n",
    "    mean_box_error = np.mean(box_errors)\n",
    "\n",
    "    # 각 포인트의 [u, v, box_error] 배열 생성\n",
    "    errors = np.stack([u_valid, v_valid, box_errors], axis=1)\n",
    "\n",
    "    return mean_box_error, errors\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "def evaluate_frame_exposure(frame: h5py.Group, frame_path: str):\n",
    "    images = read_image_pair(frame_path)\n",
    "    exposure_status = [evaluate_exposure(image) for image in images]\n",
    "    stereo_status = [evaluate_exposure_stereo(images[0], images[1]), evaluate_exposure_stereo(images[2], images[3])]\n",
    "    blur_status = [fft_blur_detection(image) for image in images]\n",
    "    if (exposure_status[0] != 0 and exposure_status[2] != 0) or stereo_status[0] != 0 or stereo_status[1] != 0:\n",
    "        frame.attrs[\"exposure_error\"] = True\n",
    "    else:\n",
    "        frame.attrs[\"exposure_error\"] = False\n",
    "\n",
    "    if blur_status[2] < 120 or blur_status[3] < 120:\n",
    "        frame.attrs[\"blur_error\"] = True\n",
    "    else:\n",
    "        frame.attrs[\"blur_error\"] = False\n",
    "\n",
    "    \n",
    "\n",
    "def update_frame_exposure_h5(h5file: str):\n",
    "    with h5py.File(h5file, \"a\") as f:\n",
    "        for frame_id in tqdm(f[\"frame\"]):\n",
    "            frame = f[\"frame\"][frame_id]\n",
    "            evaluate_frame_exposure(frame, os.path.join(os.path.dirname(h5file), frame_id))\n",
    "        f.close()    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils.points import lidar_points_to_disparity_with_cal, refine_disparity_points\n",
    "from myutils.hy5py import  read_calibration, get_frame_by_path\n",
    "from myutils.image_process import read_image_pair\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_frame(frame_path: str):\n",
    "    t_mtx = np.load(\"jai_transform.npy\")\n",
    "    cal = read_calibration(os.path.dirname(frame_path) + \"/0.hdf5\")\n",
    "    images = read_image_pair(frame_path)\n",
    "    with get_frame_by_path(frame_path) as frame:\n",
    "        if \"rgb_exposure_left\" in frame[\"image\"].attrs:\n",
    "            for s in [\"rgb\",\"nir\"]:\n",
    "                for d in [\"left\",\"right\"]:\n",
    "                    if f\"{s}_exposure_{d}\" in frame[\"image\"].attrs:\n",
    "                        print(f\"{s} {d} exposure: \", frame[\"image\"].attrs[f\"{s}_exposure_{d}\"])\n",
    "        lidar_disparity = lidar_points_to_disparity_with_cal(frame[\"lidar/points\"][:], t_mtx , cal)\n",
    "        lidar_disparity = refine_disparity_points(torch.from_numpy(lidar_disparity),).numpy()\n",
    "        if \"disparity/rgb\" in frame:\n",
    "            disparity_rgb = frame[\"disparity/rgb\"][:]\n",
    "            disparity_nir = frame[\"disparity/nir\"][:]\n",
    "\n",
    "        \n",
    "    lidar_error, lidar_e = lidar_box_error(lidar_disparity , disparity_rgb)\n",
    "    lidar_nir_error, _ = lidar_box_error(lidar_disparity , disparity_nir)\n",
    "    exposure_times = [i.mean() for i in images]\n",
    "    print(exposure_times)\n",
    "    exposure_status = [evaluate_exposure(image) for image in images]\n",
    "    stereo_status = [evaluate_exposure_stereo(images[0], images[1]), evaluate_exposure_stereo(images[2], images[3])]\n",
    "    blur_status = [fft_blur_detection(image) for image in images]\n",
    "    print(\"Lidar Box Error \",lidar_error, lidar_nir_error)\n",
    "    print(blur_status)\n",
    "    for i, exposure_status in enumerate(exposure_status):\n",
    "        print(f\"{'rgb' if i//2 == 0 else 'nir'} {'left' if i%2 == 0 else 'right'}: \", end=\"\")\n",
    "        if exposure_status == -1:\n",
    "            print(\"Underexposed\")\n",
    "        elif exposure_status == 1:\n",
    "            print(\"Overexposed\")\n",
    "        else:\n",
    "            print(\"Normal\")\n",
    "    for i, stereo_status in enumerate(stereo_status):\n",
    "        print(f\"{'rgb' if i == 0 else 'nir'} stereo: \", end=\"\")\n",
    "        if stereo_status == -1:\n",
    "            print(\"Right Overexposed\")\n",
    "        elif stereo_status == 1:\n",
    "            print(\"Left Overexposed\")\n",
    "        else:\n",
    "            print(\"Normal\")\n",
    "            \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(disparity_rgb, cmap=\"rainbow\", vmin=0, vmax=32)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(disparity_nir, cmap=\"rainbow\", vmin=0, vmax=32)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(images[0])\n",
    "    plt.scatter(lidar_e[:,0], lidar_e[:,1], c=lidar_e[:,2], cmap=\"rainbow\", s=0.5, vmin=0, vmax=8)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee1958417e540319cd3445bf8030732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Scene:', options=('09-05-17-07-36', '09-05-17-10-57', '09-05-17-27-03', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<myutils.widget.FrameExplorer at 0x7f6ed48113f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from myutils.widget import FrameExplorer\n",
    "\n",
    "\n",
    "FrameExplorer(evaluate_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
