{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "try:\n",
    "    from core.raft_stereo import RAFTStereo\n",
    "except ImportError:\n",
    "    import os\n",
    "    os.chdir(\"/RAFT-Stereo\")\n",
    "    from core.raft_stereo import RAFTStereo\n",
    "    \n",
    "FRPASS = \"frames_cleanpass\"\n",
    "from train_fusion.dataloader import StereoDataset, StereoDatasetArgs\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from fusion_args import FusionArgs\n",
    "args = FusionArgs()\n",
    "args.hidden_dims = [128, 128, 128]\n",
    "args.corr_levels = 4\n",
    "args.corr_radius = 4\n",
    "args.n_downsample = 3\n",
    "args.context_norm = \"batch\"\n",
    "args.n_gru_layers = 2\n",
    "args.shared_backbone = False\n",
    "args.mixed_precision = True\n",
    "args.corr_implementation = \"reg_cuda\"\n",
    "args.slow_fast_gru = False\n",
    "args.restore_ckpt = \"models/raftstereo-realtime.pth\"\n",
    "args.shared_backbone = True\n",
    "\n",
    "args.lr = 0.001\n",
    "args.train_iters = 15\n",
    "args.valid_iters = 24\n",
    "args.wdecay = 0.0001\n",
    "args.num_steps = 100000\n",
    "args.valid_steps = 1000\n",
    "args.name = \"ColorFusion\"\n",
    "args.batch_size = 8\n",
    "args.fusion = \"AFF\"\n",
    "args.shared_fusion = True\n",
    "args.freeze_backbone = []\n",
    "args.both_side_train= False\n",
    "\n",
    "device = \"cuda:5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from color_fusion_model import RGBNIRFusionNet\n",
    "from rgb_thermal_fusion_net import RGBThermalFusionNet\n",
    "\n",
    "from encoding.parallel import DataParallelModel, DataParallelCriterion\n",
    "args.input_channel = 3\n",
    "\n",
    "raft_model = DataParallelModel(RAFTStereo(args)).cuda()\n",
    "raft_model.load_state_dict(torch.load(args.restore_ckpt))\n",
    "raft_model.eval()\n",
    "raft_model.module.freeze_bn()\n",
    "raft_model = raft_model.module\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparity(left: torch.Tensor, right: torch.Tensor):\n",
    "        if left.shape[-3] == 1:\n",
    "            left = left.repeat(1, 3, 1, 1)\n",
    "            right = right.repeat(1, 3, 1, 1)\n",
    "        _, flow = raft_model(left, right, test_mode=True)\n",
    "        return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_fusion.my_h5_dataloader import MyH5DataSet\n",
    "from train_fusion.dataloader import StereoDataset, StereoDatasetArgs, EntityDataSet\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "dataset_real = MyH5DataSet( frame_cache=True,use_right_shift = True)\n",
    "print(len(dataset_real))\n",
    "dataset_real = MyH5DataSet( frame_cache=False,use_right_shift = True)\n",
    "print(len(dataset_real))\n",
    "\n",
    "\n",
    "dataset_drive = StereoDataset(StereoDatasetArgs(flow3d_driving_json=True, noised_input=True, shift_filter=True, vertical_scale=True, fast_test=True))\n",
    "dataset_flying = StereoDataset(StereoDatasetArgs(flying3d_json=True, noised_input=True, shift_filter=True, fast_test=True))\n",
    "dataset = EntityDataSet(dataset_real.input_list + dataset_drive.input_list + dataset_flying.input_list)\n",
    "cnt = len(dataset)\n",
    "train_cnt = int(cnt * 0.9)\n",
    "valid_cnt = cnt - train_cnt\n",
    "print(cnt)\n",
    "dataset_train = EntityDataSet(dataset.input_list[:train_cnt])\n",
    "dataset_valid =EntityDataSet(dataset.input_list[train_cnt:])\n",
    "train_loader = DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver_loader = DataLoader(dataset_drive, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "flying_loader = DataLoader(dataset_flying, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "print(len(dataset_drive))\n",
    "print(len(dataset_flying))\n",
    "print(len(dataset_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(dataset_real, batch_size=1, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from train_fusion.loss_function import reproject_disparity\n",
    "\n",
    "def input_reduce_disparity(inputs: list[torch.Tensor]):\n",
    "    shift = int(inputs[-1].min() // 16) * 16\n",
    "    if shift <= 0:\n",
    "        return inputs\n",
    "    warp_right = reproject_disparity(-inputs[-1] + shift, data[0])\n",
    "    warp_right_nir = reproject_disparity(-inputs[-1] + shift, data[2])\n",
    "    rolled_rgb_right = torch.roll(inputs[1], shifts = shift, dims = -1)\n",
    "    rolled_nir_right = torch.roll(inputs[3], shifts = shift, dims = -1)\n",
    "    rolled_rgb_right[...,:shift] = warp_right[...,:shift]\n",
    "    rolled_nir_right[...,:shift] = warp_right_nir[...,:shift]\n",
    "    \n",
    "    inputs[-1] -= shift\n",
    "    inputs[-2][...,:2] -= shift\n",
    "    return [\n",
    "        inputs[0], rolled_rgb_right, inputs[2], rolled_nir_right, inputs[-2], inputs[-1]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def crop_and_resize_height(image: torch.Tensor, h_to = 360):\n",
    "    b, c, h, w = image.shape\n",
    "    print(image.shape)\n",
    "    cropped = image[:,:,:h_to,:]\n",
    "    resized = F.interpolate(cropped, size=(h, w), mode='bilinear',)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_fusion.loss_function import reproject_disparity\n",
    "from train_fusion.ssim.utils import SSIM, warp\n",
    "\n",
    "#data = next(iter(valid_loader))\n",
    "idx = 33\n",
    "# dataset_drive.input_list[idx].noise_target = \"nir\"\n",
    "# dataset_drive.input_list[idx].guided_noise = 15\n",
    "# dataset_drive.input_list[idx].gamma_noise = 3\n",
    "# dataset_drive.input_list[idx].shift_filter = True\n",
    "\n",
    "\n",
    "data = dataset_real.input_list[idx].get_item()\n",
    "data = [x.unsqueeze(0) for x in data]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(321)\n",
    "plt.title(\"GT Disparity\")\n",
    "sc = plt.imshow(data[-1][0,0].numpy(), vmin=0, vmax=64, cmap=\"rainbow\")\n",
    "plt.colorbar(sc)\n",
    "plt.subplot(322)\n",
    "plt.title(\"Left RGB\")\n",
    "plt.imshow(data[0][0].permute(1,2,0).numpy().astype(np.uint8))\n",
    "plt.subplot(324)\n",
    "plt.title(\"Right RGB\")\n",
    "plt.imshow(data[1][0].permute(1,2,0).numpy().astype(np.uint8))\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.title(\"Left NIR\")\n",
    "plt.imshow(data[2][0].permute(1,2,0).numpy().astype(np.uint8), cmap=\"gray\")\n",
    "plt.subplot(326)\n",
    "plt.title(\"Right NIR\")\n",
    "plt.imshow(data[3][0].permute(1,2,0).numpy().astype(np.uint8), cmap=\"gray\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    flow = raft_model(data[2].repeat(1,3,1,1).cuda(), data[3].repeat(1,3,1,1).cuda(), test_mode=True)[1][:,:,:540,:720]\n",
    "    disp = -flow[0,0]\n",
    "    right = torch.concat((data[1], data[3]), dim = 1).cuda() / 255\n",
    "    warp_right = warp(torch.concat([data[0], data[2]] , dim = 1).cuda() / 255, flow)\n",
    "    mask_right = warp(torch.ones_like( right).cuda(), flow, padding_mode=\"zeros\")\n",
    "    ssim_loss = SSIM()(warp_right, right)\n",
    "    l1_loss = torch.abs(warp_right - right )\n",
    "    loss = (ssim_loss * 0.85 + 0.15 * l1_loss.mean(1, True))[mask_right > 0]\n",
    "    \n",
    "    print(loss)\n",
    "    print(loss.mean())\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.title(\"RAFT Stereo\")\n",
    "sc =plt.imshow(disp.cpu().numpy(), vmin=0, vmax=64, cmap=\"rainbow\")\n",
    "plt.colorbar(sc)\n",
    "plt.subplot(325)\n",
    "plt.imshow(warp_right[0].permute(1,2,0).cpu().numpy()[...,:3])\n",
    "# plt.subplot(326)\n",
    "# plt.imshow(crop_and_resize_height(data[5])[0].permute(1,2,0).numpy(), cmap=\"plasma\", vmin=0, vmax=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgb_thermal_fusion_net import RGBThermalFusionNet\n",
    "import matplotlib.pyplot as plt\n",
    "from color_fusion_model import RGBNIRFusionNet\n",
    "\n",
    "\n",
    "def loss_fn_detph_gt(flow: torch.Tensor, target_gt: torch.Tensor):\n",
    "    gt_u = target_gt[:, :, 1].long()\n",
    "    gt_v = target_gt[:, :, 0].long()\n",
    "    gt_u = torch.clamp(gt_u, 0, flow.shape[-2] - 1)\n",
    "    gt_v = torch.clamp(gt_v, 0, flow.shape[-1] - 1)\n",
    "    B, N = gt_u.shape\n",
    "    batch_indices = torch.arange(B).view(B, 1).expand(B, N).to(flow.device)\n",
    "    target_pred = -flow[batch_indices, :, gt_u, gt_v].squeeze()\n",
    "\n",
    "    target_depth = target_gt[:, :, 2]\n",
    "    depth_loss = torch.mean(torch.abs(target_pred - target_depth), dim=1)\n",
    "\n",
    "    return depth_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.raft_stereo import RAFTStereo\n",
    "fusion_args = FusionArgs()\n",
    "fusion_args.batch_size = 3\n",
    "fusion_args.input_channel = 4\n",
    "fusion_args.corr_levels = 4\n",
    "fusion_args.corr_radius = 4\n",
    "fusion_args.n_downsample = 2\n",
    "fusion_args.context_norm = \"batch\"\n",
    "fusion_args.n_gru_layers = 3\n",
    "fusion_args.shared_backbone = False\n",
    "fusion_args.train_iters = 15\n",
    "fusion_args.valid_iters = 20\n",
    "model_RAFT = RAFTStereo(fusion_args).cuda()\n",
    "model_RAFT.load_state_dict(torch.load(\"checkpoints/2664_Raft4ChannelRaftLoss2.pth\"))\n",
    "#model.load_state_dict(torch.load(\"interrupted_model.pth\"))\n",
    "model_RAFT.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from color_fusion_model import RGBNIRFusionNet\n",
    "model = RGBNIRFusionNet().cuda()\n",
    "model.load_state_dict(torch.load(\"checkpoints/2200_ColorChannel4.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.raft_stereo_fusion import RAFTStereoFusion\n",
    "fusion_args = FusionArgs()\n",
    "fusion_args.n_downsample = 2\n",
    "fusion_args.n_gru_layers = 3\n",
    "fusion_args.shared_backbone = False\n",
    "fusion_args.shared_fusion = True\n",
    "model_AFF = RAFTStereoFusion(fusion_args).cuda()\n",
    "model_AFF.eval()\n",
    "fusion_args = FusionArgs()\n",
    "fusion_args.n_downsample = 3\n",
    "fusion_args.n_gru_layers = 2\n",
    "fusion_args.shared_backbone = True\n",
    "fusion_args.shared_fusion = True\n",
    "model_AFF_F = RAFTStereoFusion(fusion_args).cuda()\n",
    "model_AFF_F.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def create_overlay_image(original_image, feature_map, num_channels=8, alpha=0.6, cmap='jet', vmin=0, vmax=1):\n",
    "    \"\"\"\n",
    "    original_image: Tensor of shape (3, H, W)\n",
    "    feature_map: Tensor of shape (256, H//8, W//8)\n",
    "    num_channels: Number of feature map channels to aggregate\n",
    "    alpha: Transparency for heatmap overlay (0 to 1)\n",
    "    cmap: Colormap for heatmap\n",
    "    Returns:\n",
    "        overlay_image: NumPy array of shape (H, W, 3) with values in [0, 1]\n",
    "    \"\"\"\n",
    "    # 선택할 채널 수 조정\n",
    "    num_channels = min(num_channels, feature_map.shape[0])\n",
    "    \n",
    "    # 첫 num_channels 채널 선택 (필요에 따라 중요 채널 선택 로직 추가 가능)\n",
    "    selected_features = feature_map[:num_channels, :, :]\n",
    "    \n",
    "    # feature map을 평균하여 단일 채널로 축소\n",
    "    aggregated_feature = torch.mean(selected_features, dim=0, keepdim=True)  # (1, H//8, W//8)\n",
    "    \n",
    "    # 업샘플링하여 원본 이미지 크기로 복원\n",
    "    upsampled_feature = F.interpolate(aggregated_feature.unsqueeze(0), size=(original_image.shape[1], original_image.shape[2]), mode='bilinear', align_corners=False)\n",
    "    upsampled_feature = upsampled_feature.squeeze().detach().cpu().numpy()  # (H, W)\n",
    "    \n",
    "    upsampled_feature -= vmin\n",
    "    \n",
    "    upsampled_feature /= (vmax - vmin)\n",
    "    #upsampled_feature = np.clip(upsampled_feature, vmin, vmax)\n",
    "    \n",
    "    # 컬러맵 적용\n",
    "    import matplotlib.cm as cm\n",
    "    cmap_func = cm.get_cmap(cmap)\n",
    "    heatmap = cmap_func(upsampled_feature)[:, :, :3]  # (H, W, 3), RGB\n",
    "    \n",
    "    # 원본 이미지 변환 (Tensor -> NumPy, 채널 순서 변경)\n",
    "    original_np = original_image.detach().cpu().numpy()\n",
    "    original_np = np.transpose(original_np, (1, 2, 0))  # (H, W, 3)\n",
    "    \n",
    "    original_np /= 255.0\n",
    "    \n",
    "    # Heatmap과 원본 이미지 결합\n",
    "    overlay_image = (1 - alpha) * original_np + alpha * heatmap\n",
    "    overlay_image = np.clip(overlay_image, 0, 1)\n",
    "    \n",
    "    return overlay_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from myutils.hy5py import calibration_property, get_frame_by_path, read_calibration\n",
    "from myutils.matrix import rmse_loss\n",
    "from train_fusion.loss_function import (\n",
    "    self_supervised_loss,\n",
    "    reproject_disparity,\n",
    "    disparity_smoothness,\n",
    "    ssim as ssim_torch,\n",
    ")\n",
    "from myutils.points import (\n",
    "    disparity_image_edge_eval,\n",
    "    project_points_on_camera,\n",
    "    refine_disparity_points,\n",
    "    transform_point_inverse,\n",
    ")\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "cmap_disparity = \"rainbow\"\n",
    "spectrum = [\"RGB\", \"NIR\", \"FUSION\"]\n",
    "\n",
    "\n",
    "def plot_raft_model(x, model_type=\"Raft\", ckpoints=None, fusion_only=False, C=0):\n",
    "    train_inputs = [x.cuda() for x in x]\n",
    "    image1, image2, image3, image4 = train_inputs[:4]\n",
    "\n",
    "    if C > 0:\n",
    "        image2 = torch.roll(image2, shifts=-C, dims=-1)\n",
    "        image4 = torch.roll(image4, shifts=-C, dims=-1)\n",
    "\n",
    "    if len(train_inputs) > 4:\n",
    "        depth = train_inputs[4]\n",
    "        if C > 0:\n",
    "            depth[..., 2] += C\n",
    "    else:\n",
    "        depth = None\n",
    "    if len(train_inputs) > 5:\n",
    "        dis_gt = train_inputs[5]\n",
    "    else:\n",
    "        dis_gt = None\n",
    "    with torch.no_grad():\n",
    "        if model_type in [\"AFF\", \"AFF_F\"]:\n",
    "            model = model_AFF if model_type == \"AFF\" else model_AFF_F\n",
    "            if ckpoints is not None:\n",
    "                model.load_state_dict(torch.load(ckpoints), strict=False)\n",
    "\n",
    "            input_dict = {\n",
    "                \"image_viz_left\": image1,\n",
    "                \"image_viz_right\": image2,\n",
    "                \"image_nir_left\": image3,\n",
    "                \"image_nir_right\": image4,\n",
    "                \"iters\": args.valid_iters,\n",
    "                \"test_mode\": True,\n",
    "                \"flow_init\": None,\n",
    "                \"heuristic_nir\": False,\n",
    "                \"attention_out_mode\": False,\n",
    "            }\n",
    "            _, flow = model(input_dict)\n",
    "            flow += C\n",
    "            input_dict[\"attention_out_mode\"] = True\n",
    "            fmap1, fmap1_rgb, fmap1_nir = model(input_dict)\n",
    "\n",
    "        if model_type == \"RAFT\":\n",
    "            model = model_RAFT\n",
    "            if ckpoints is not None:\n",
    "                model.load_state_dict(torch.load(ckpoints))\n",
    "            fused_input1 = torch.cat(\n",
    "                [image1, image3], dim=1\n",
    "            )  # image1: RGB, image3: Thermal\n",
    "            fused_input2 = torch.cat([image2, image4], dim=1)\n",
    "\n",
    "            coor, flow = model(fused_input1, fused_input2, test_mode=True, iters=24)\n",
    "            fmap1, fmap2 = model.fnet([fused_input1, fused_input2])\n",
    "\n",
    "        if model_type == \"Color\":\n",
    "            fused_input1 = torch.cat(\n",
    "                [image1, image3], dim=1\n",
    "            )  # image1: RGB, image3: Thermal\n",
    "            fused_input2 = torch.cat([image2, image4], dim=1)\n",
    "            fused_image1 = model(fused_input1)\n",
    "            fused_image2 = model(fused_input2)\n",
    "            fmap1, fmap2 = raft_model.fnet([fused_image1, fused_image2])\n",
    "\n",
    "            _, flow = raft_model(fused_image1, fused_image2, test_mode=True, iters=24)\n",
    "\n",
    "        if not model_type in [\"AFF\", \"AFF_F\"]:\n",
    "            fmap1_rgb, fmap2_rgb = raft_model.fnet([image1, image2])\n",
    "            fmap1_nir, fmap2_nir = raft_model.fnet(\n",
    "                [image3.repeat(1, 3, 1, 1), image4.repeat(1, 3, 1, 1)]\n",
    "            )\n",
    "        if not fusion_only:\n",
    "            coor_rgb, flow_rgb = raft_model(image1, image2, test_mode=True, iters=24)\n",
    "            _, flow_nir = raft_model(\n",
    "                image3.repeat(1, 3, 1, 1),\n",
    "                image4.repeat(1, 3, 1, 1),\n",
    "                test_mode=True,\n",
    "                iters=24,\n",
    "            )\n",
    "\n",
    "        disparity = -flow[:, 0]\n",
    "        disparity = disparity.cpu().numpy()\n",
    "        if not fusion_only:\n",
    "            disparity_rgb = -flow_rgb[:, 0].cpu().numpy() - C\n",
    "            disparity_nir = -flow_nir[:, 0].cpu().numpy() - C\n",
    "\n",
    "    batch_size = image1.shape[0]\n",
    "\n",
    "    def plt_disparity(ax, disparity, title, vmax):\n",
    "        sc = ax.imshow(disparity, cmap=cmap_disparity, vmin=0, vmax=vmax)\n",
    "        ax.set_title(title)\n",
    "        plt.colorbar(sc, ax=ax)\n",
    "\n",
    "    def plt_featuremap(ax, image, feature_map, title):\n",
    "        overlay = create_overlay_image(image, feature_map, alpha=0.7, vmin=0, vmax=1)\n",
    "        ax.imshow(overlay)\n",
    "        ax.set_title(title)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        np_rgb_left, np_rgb_right, np_nir_left, np_nir_right = [\n",
    "            x[b].permute(1, 2, 0).cpu().numpy().astype(np.uint8) for x in train_inputs[:4]\n",
    "        ]\n",
    "        np_fusion_left = modify_v_channel_numpy_opencv(np_rgb_left.copy(), np_nir_left.copy()) * 255\n",
    "        np_fusion_right = modify_v_channel_numpy_opencv(np_rgb_right.copy(), np_nir_right.copy()) * 255\n",
    "        np_fusion_left = guided_filter(np_nir_left.squeeze(), np_fusion_left, 5)\n",
    "        np_fusion_right = guided_filter(np_nir_right.squeeze(), np_fusion_right, 5)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            disparity_hsv = -raft_model(\n",
    "                torch.from_numpy(np_fusion_left).permute(2, 0, 1).unsqueeze(0).cuda(),\n",
    "                torch.from_numpy(np_fusion_right).permute(2, 0, 1).unsqueeze(0).cuda(),\n",
    "                test_mode=True,\n",
    "            )[1][0, 0]\n",
    "        print(np_fusion_left.max())\n",
    "        rows = 4\n",
    "        cols = 1 if fusion_only else 3\n",
    "        if depth is not None:\n",
    "            cols += 1\n",
    "        vmax = disparity[b].mean() * 2.2\n",
    "        vmax = min(128, vmax)\n",
    "        fig, axs = plt.subplots(cols, rows, figsize=(8 * rows, 5 * cols))\n",
    "        if not fusion_only:\n",
    "            axs[0, 0].imshow(np_rgb_left.astype(np.uint8))\n",
    "            axs[0, 0].set_title(\"RGB_Left\")\n",
    "\n",
    "            axs[1, 0].imshow(np_nir_left.astype(np.uint8), cmap=\"gray\")\n",
    "            axs[1, 0].set_title(\"NIR_Left\")\n",
    "            plt_disparity(axs[0, 2], disparity_rgb[b], \"Disparity_RGB_RaftStereo\", vmax)\n",
    "            plt_disparity(axs[1, 2], disparity_nir[b], \"Disparity_NIR_RaftStereo\", vmax)\n",
    "            plt_disparity(axs[2, 2], disparity[b], \"Disparity_Fused\", vmax)\n",
    "        else:\n",
    "            plt_disparity(axs[1], disparity[b], \"Disparity_Fused\", vmax)\n",
    "        plt_featuremap(axs[0, 3], image1[b], fmap1_rgb[b], \"RGB Attention\")\n",
    "        plt_featuremap(axs[1, 3], image3[b], fmap1_nir[b], \"NIR Attention\")\n",
    "        plt_featuremap(axs[2, 3], image1[b], fmap1[b], \"Fused Attention\")\n",
    "\n",
    "        if model_type == \"Color\":\n",
    "            axs[2, 0].set_title(\"Fused Image\")\n",
    "            fused = (\n",
    "                fused_image1[b].permute(1, 2, 0).cpu().numpy()\n",
    "                / fused_image1[b].max()\n",
    "                * 255\n",
    "            )\n",
    "            axs[2, 0].imshow(fused.astype(np.uint8))\n",
    "\n",
    "        lidar_depth = depth[b].cpu().numpy()\n",
    "        u, v = lidar_depth[:, 0], lidar_depth[:, 1]\n",
    "        z = lidar_depth[:, 2]\n",
    "\n",
    "        def plot_points(\n",
    "            ax, title, image, u, v, z, vmax=32, vmin=0, cmap=cmap_disparity\n",
    "        ):\n",
    "            ax.imshow(image.permute(1, 2, 0).cpu().numpy().astype(np.uint8))\n",
    "            sc = ax.scatter(u, v, c=z, cmap=cmap, vmin=vmin, vmax=vmax, s=1)\n",
    "            ax.set_title(title)\n",
    "            plt.colorbar(sc, ax=ax)\n",
    "\n",
    "        u = u.astype(np.int32)\n",
    "        v = v.astype(np.int32)\n",
    "        u_r = (u - z).astype(np.int32)\n",
    "        u_r[u_r < 0] = 0\n",
    "        color_sampled = np.clip(np_rgb_right[v, u_r] / 255.0, 0, 1)\n",
    "        axs[2, 0].imshow(np.zeros_like(disparity[b]), cmap=\"gray\")\n",
    "        axs[2, 0].scatter(u, v, c=color_sampled, s=10)\n",
    "        axs[2, 0].set_title(\"Lidar Warped\")\n",
    "        # plot_points(axs[2, 0], \"Lidar Disparity\", image1[b], u, v, z, vmax)\n",
    "\n",
    "        dis_losses = [\n",
    "            rmse_loss(z, x[v, u])\n",
    "            for x in [disparity_rgb[b], disparity_nir[b], disparity[b]]\n",
    "        ]\n",
    "        smooth_losses = [\n",
    "            disparity_smoothness([x], torch.concat([image1, image3], dim=1))\n",
    "            .mean()\n",
    "            .cpu()\n",
    "            for x in [flow_rgb, flow_nir, flow]\n",
    "        ]\n",
    "        edge_loss = [\n",
    "            disparity_image_edge_eval(x, np_rgb_left.astype(np.uint8))\n",
    "            for x in [\n",
    "                disparity_rgb[b, :540, :720],\n",
    "                disparity_nir[b, :540, :720],\n",
    "                disparity[b, :540, :720],\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        labels = [\"rgb\", \"nir\", \"fusion\"]\n",
    "        colors = [\"blue\", \"green\", \"orange\"]\n",
    "\n",
    "        left_concat = torch.concatenate([image1[b : b + 1], image3[b : b + 1]], 1)\n",
    "        right_concat = torch.concatenate([image2[b : b + 1], image4[b : b + 1]], 1)\n",
    "        warped_right = [\n",
    "            reproject_disparity(\n",
    "                flow[b : b + 1],\n",
    "                left_concat,\n",
    "            )\n",
    "            for flow in [flow_rgb, flow_nir, flow]\n",
    "        ]\n",
    "        warped_right_np = [x[0].permute(1, 2, 0).cpu().numpy() for x in warped_right]\n",
    "\n",
    "        axs[0, 1].imshow(np_rgb_right.astype(np.uint8))\n",
    "        axs[0, 1].set_title(\"RGB Right\")\n",
    "        axs[1, 1].imshow(np_nir_right.astype(np.uint8), cmap=\"gray\")\n",
    "        axs[1, 1].set_title(\"NIR Right\")\n",
    "\n",
    "        ssim_list = [\n",
    "            1\n",
    "            - ssim_torch(right_concat, warped_right)\n",
    "            .cpu()[0]\n",
    "            .permute(1, 2, 0)\n",
    "            .numpy()\n",
    "            .mean(axis=-1)\n",
    "            for warped_right in warped_right\n",
    "        ]\n",
    "\n",
    "        # for ci, img in enumerate(ssim_list[: 2 if dis_gt is not None else 3]):\n",
    "        #     sc = axs[ci, 3].imshow(img, cmap=\"OrRd\", vmax=1, vmin=0)\n",
    "        #     axs[ci, 3].set_title(f\"{spectrum[ci]} Warp Error\")\n",
    "        #     plt.colorbar(sc, ax=axs[ci, 3])\n",
    "        if dis_gt is not None and dis_gt.max() > 5:\n",
    "            sc = axs[2, 1].imshow(\n",
    "                dis_gt[b, 0].cpu(), cmap=cmap_disparity, vmin=0, vmax=vmax\n",
    "            )\n",
    "            axs[2, 1].set_title(\"Disparity GT\")\n",
    "            plt.colorbar(sc, ax=axs[2, 1])\n",
    "        else:\n",
    "            axs[2, 1].imshow(np_nir_left, cmap=\"gray\")\n",
    "            sc = axs[2, 1].scatter(u, v, c=z, s=0.1, cmap=cmap_disparity)\n",
    "            plt.colorbar(sc, ax=axs[2, 1])\n",
    "            axs[2, 1].set_title(\"Lidar Points\")\n",
    "\n",
    "        ssim_losses = [x.mean() for x in ssim_list]\n",
    "        # for i, (data, title, label) in enumerate(\n",
    "        #     [\n",
    "        #         (dis_losses, \"Disparity RMSE Loss Comparison\", \"RMSE Error\"),\n",
    "        #         (smooth_losses, \"Disparity Smoothness Comparison\", \"Smoothness\"),\n",
    "        #         (edge_loss, \"Edge Comparison\", \"Edge RMSE\"),\n",
    "        #         (ssim_losses, \"SSIM Comparision\", \"Warp SSIM\"),\n",
    "        #     ]\n",
    "        # ):\n",
    "\n",
    "        #     vgap = max(data) - min(data)\n",
    "        #     if vgap > min(data):\n",
    "        #         vgap = min(data)\n",
    "        #     ymax = max(data) + vgap * 0.3\n",
    "        #     axs[3, i].bar(labels, data, color=colors)\n",
    "        #     axs[3, i].set_ylabel(label)\n",
    "        #     axs[3, i].set_ylim(float(min(data) - vgap), float(ymax))\n",
    "        #     axs[3, i].set_title(title)\n",
    "        axs[3, 0].imshow(np_fusion_left.astype(np.uint8))\n",
    "        axs[3, 1].imshow(np_fusion_right.astype(np.uint8))\n",
    "        axs[3, 2].imshow(disparity_hsv.cpu(), vmin=0, vmax=vmax, cmap=cmap_disparity)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def imread_tensor(image_path: str):\n",
    "    nir = \"nir\" in image_path\n",
    "    image = cv2.imread(\n",
    "        image_path, cv2.IMREAD_ANYCOLOR if not nir else cv2.IMREAD_GRAYSCALE\n",
    "    )\n",
    "    if not nir:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = torch.from_numpy(image)\n",
    "    if nir:\n",
    "        image = image.unsqueeze(-1)\n",
    "    image = image.permute(2, 0, 1)\n",
    "    image = image.unsqueeze(0).float()\n",
    "    return image\n",
    "\n",
    "\n",
    "from myutils.image_process import (\n",
    "    guided_filter,\n",
    "    modify_v_channel_numpy_opencv,\n",
    "    read_image_pair,\n",
    "    cv2toTensor,\n",
    ")\n",
    "\n",
    "\n",
    "def get_valid_input_from_path(frame_path: str):\n",
    "    transform_mtx = np.load(\"jai_transform.npy\")\n",
    "    images = read_image_pair(frame_path)\n",
    "    images = [cv2toTensor(x).cuda() for x in images]\n",
    "    calibration = read_calibration(\"/bean/depth/09-08-17-27-33/0.hdf5\")\n",
    "    fx, bs, cx, cy = calibration_property(calibration)\n",
    "    with get_frame_by_path(frame_path) as frame:\n",
    "        lidar_points = (frame[\"lidar/points\"][:] * 1000).reshape(-1, 3)\n",
    "        lidar_points = transform_point_inverse(lidar_points, transform_mtx)\n",
    "        lidar_points = project_points_on_camera(lidar_points, fx, cx, cy, 720, 540)\n",
    "        lidar_points[:, 2] = bs * fx / lidar_points[:, 2] - 1\n",
    "        lidar_points = refine_disparity_points(torch.from_numpy(lidar_points)).numpy()\n",
    "        if \"rgb_exposure_left\" in frame[\"image\"].attrs:\n",
    "            print(\n",
    "                frame[\"image\"].attrs[\"rgb_exposure_left\"],\n",
    "                frame[\"image\"].attrs[\"rgb_exposure_right\"],\n",
    "                frame[\"image\"].attrs[\"nir_exposure_left\"],\n",
    "                frame[\"image\"].attrs[\"nir_exposure_right\"],\n",
    "            )\n",
    "    images.append(torch.from_numpy(lidar_points).cuda().unsqueeze(0))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_iter = iter(valid_loader)\n",
    "train_input = [x.unsqueeze(0) for x in dataset_real[466]]\n",
    "plot_raft_model(train_input, \"AFF\", \"checkpoints/10323_RaftRFusion2NoiseReal.pth\")\n",
    "#plot_raft_model(train_input, \"AFF_F\", \"checkpoints/10292_RaftRFusion2SynthFast.pth\")\n",
    "#plot_raft_model(train_input, \"RAFT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils.widget import FrameExplorer\n",
    "\n",
    "FrameExplorer(lambda x : plot_raft_model(get_valid_input_from_path(x), model_type= \"AFF\", C=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameExplorer(lambda x : plot_raft_model(get_valid_input_from_path(x), model_type= \"AFF_F\", C=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_path = \"/bean/depth/09-08-17-27-33/17_33_16_354\"\n",
    "frame_path = \"/bean/depth/09-08-17-27-33/17_32_56_438\"\n",
    "#frame_path = \"/bean/depth/09-28-21-15-50/21_34_03_125\"\n",
    "#plot_raft_model(get_valid_input_from_path(frame_path), model_type = \"AFF\", ckpoints=\"checkpoints/4000_RaftFusion22EncoderLarge.pth\")\n",
    "plot_raft_model(get_valid_input_from_path(frame_path), model_type = \"AFF\", C =0)\n",
    "#plot_raft_model(get_valid_input_from_path(frame_path), model_type = \"RAFT\", ckpoints = 'checkpoints/666_Raft4ChannelRaftSynth.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
