{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "try:\n",
    "    from core.raft_stereo import RAFTStereo\n",
    "except ImportError:\n",
    "    import os\n",
    "    os.chdir(\"/RAFT-Stereo\")\n",
    "    from core.raft_stereo import RAFTStereo\n",
    "    \n",
    "FRPASS = \"frames_cleanpass\"\n",
    "from train_fusion.dataloader import StereoDataset, StereoDatasetArgs\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from fusion_args import FusionArgs\n",
    "args = FusionArgs()\n",
    "args.hidden_dims = [128, 128, 128]\n",
    "args.corr_levels = 4\n",
    "args.corr_radius = 4\n",
    "args.n_downsample = 3\n",
    "args.context_norm = \"batch\"\n",
    "args.n_gru_layers = 2\n",
    "args.shared_backbone = True\n",
    "args.mixed_precision = True\n",
    "args.corr_implementation = \"reg_cuda\"\n",
    "args.slow_fast_gru = False\n",
    "args.restore_ckpt = \"models/raftstereo-realtime.pth\"\n",
    "\n",
    "\n",
    "args.lr = 0.001\n",
    "args.train_iters = 7\n",
    "args.valid_iters = 12\n",
    "args.wdecay = 0.0001\n",
    "args.num_steps = 100000\n",
    "args.valid_steps = 1000\n",
    "args.name = \"ColorFusion\"\n",
    "args.batch_size = 4\n",
    "args.fusion = \"AFF\"\n",
    "args.shared_fusion = True\n",
    "args.freeze_backbone = []\n",
    "args.both_side_train= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raft_model = torch.nn.DataParallel(RAFTStereo(args)).cuda()\n",
    "raft_model.load_state_dict(torch.load(args.restore_ckpt))\n",
    "raft_model.eval()\n",
    "raft_model = raft_model.module\n",
    "raft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "def fusion_rgb_nir(rgb: np.ndarray, nir: np.ndarray):\n",
    "    gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY)\n",
    "    luminance_weight = (gray - nir) / 255\n",
    "    \n",
    "    ycrcb = cv2.cvtColor(rgb, cv2.COLOR_BGR2YCrCb)\n",
    "    ycrcb_l = ycrcb[:, :, 0]\n",
    "    ycrcb_l_sum = ycrcb_l * luminance_weight + nir * (1 - luminance_weight)\n",
    "    ycrcb_a = ycrcb[:, :, 1]\n",
    "    ycrcb_b = ycrcb[:, :, 2]\n",
    "    m = (ycrcb_l - ycrcb_l_sum) / ycrcb_l\n",
    "    \n",
    "    ycrcb_a_new = ycrcb_a * (1+m)\n",
    "    ycrcb_b_new = ycrcb_b * (1+m)\n",
    "    ycrcb_new = np.stack([ycrcb_l_sum, ycrcb_a_new, ycrcb_b_new], axis=-1)\n",
    "    ycrcb_new = np.clip(ycrcb_new, 0, 255).astype(np.uint8)\n",
    "    fusion = cv2.cvtColor(ycrcb_new, cv2.COLOR_YCrCb2BGR)\n",
    "    return fusion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frame = \"/bean/depth/09-10-15-21-40/15_21_43_545\"\n",
    "left = cv2.imread(f\"{frame}/rgb/left.png\")\n",
    "left_nir = cv2.imread(f\"{frame}/nir/left.png\", cv2.IMREAD_GRAYSCALE)\n",
    "fusion = fusion_rgb_nir(left, left_nir)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(131)\n",
    "plt.imshow(cv2.cvtColor(left, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(132)\n",
    "plt.imshow(cv2.cvtColor(left_nir, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(133)\n",
    "plt.imshow(cv2.cvtColor(fusion, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_fusion.my_h5_dataloader import MyH5DataSet\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = MyH5DataSet( frame_cache=True)\n",
    "cnt = len(dataset)\n",
    "train_cnt = int(cnt * 0.9)\n",
    "valid_cnt = cnt - train_cnt\n",
    "print(cnt)\n",
    "dataset_train = MyH5DataSet(id_list = dataset.frame_id_list[:train_cnt])\n",
    "dataset_valid = MyH5DataSet(id_list = dataset.frame_id_list[train_cnt:])\n",
    "train_loader = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=args.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from color_fusion_model import RGBNIRFusionNet\n",
    "\n",
    "fusion_model = RGBNIRFusionNet().cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from train_fusion.loss_function import warp_reproject_loss, reproject_disparity\n",
    "\n",
    "def compute_disparity(left: torch.Tensor, right: torch.Tensor):\n",
    "    if left.shape[-3] == 1:\n",
    "        left = left.repeat(1, 3, 1, 1)\n",
    "        right = right.repeat(1, 3, 1, 1)\n",
    "    _, flow = raft_model(left, right, test_mode=True)\n",
    "    return flow\n",
    "\n",
    "\n",
    "def loss_fn_detph_gt(flow: torch.Tensor, target_gt: torch.Tensor):\n",
    "    gt_u = target_gt[:,:,1].long()\n",
    "    gt_v = target_gt[:,:,0].long()\n",
    "    gt_u = torch.clamp(gt_u, 0, flow.shape[-2]-1)\n",
    "    gt_v = torch.clamp(gt_v, 0, flow.shape[-1]-1)\n",
    "    B, N = gt_u.shape\n",
    "    batch_indices = torch.arange(B).view(B, 1).expand(B, N)\n",
    "    target_pred = -flow[batch_indices,:,gt_u, gt_v].squeeze()\n",
    "    \n",
    "    target_depth = target_gt[:,:, 2]\n",
    "    depth_loss = nn.MSELoss()(target_pred, target_depth)\n",
    "    \n",
    "    return depth_loss\n",
    "def loss_fn(pred: Tuple[torch.Tensor, torch.Tensor], target: Tuple[Tuple[torch.Tensor,torch.Tensor],Tuple[torch.Tensor, torch.Tensor]], target_gt: torch.Tensor):\n",
    "    flow = compute_disparity(pred[0], pred[1])\n",
    "    flow = flow[:,:,:pred[0].shape[-2], :pred[0].shape[-1]]\n",
    "\n",
    "    warp_loss_rgb, warp_metric_rgb = warp_reproject_loss([flow], *target[0])\n",
    "    warp_loss_nir, warp_metric_nir = warp_reproject_loss([flow], *target[1])\n",
    "    \n",
    "    depth_loss = loss_fn_detph_gt(flow, target_gt)\n",
    "    \n",
    "    loss_dict = {\n",
    "        **warp_metric_rgb,\n",
    "    }\n",
    "    for k, v in warp_metric_nir.items():\n",
    "        loss_dict[f\"{k}_nir\"] = v\n",
    "    loss_dict[\"depth_loss\"] = depth_loss\n",
    "    return warp_loss_rgb.mean() + warp_loss_nir.mean() + depth_loss, loss_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def validate_things(\n",
    "    model,\n",
    "    valid_loader: DataLoader,\n",
    "):\n",
    "    model.eval()\n",
    "    metrics: Dict[str, torch.Tensor] = {}\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i_batch, input_valid in enumerate(valid_loader):\n",
    "            image1, image2, image3, image4, depth = [x.cuda() for x in data_blob]\n",
    "\n",
    "            \n",
    "            image_fusion_1 = fusion_model(image1, image3)\n",
    "            image_fusion_2 = fusion_model(image2, image4)\n",
    "            \n",
    "            loss, metric = loss_fn((image_fusion_1, image_fusion_2), ((image1, image2), (image3, image4)), depth)            \n",
    "\n",
    "            print(f\"Batch {i_batch} Loss {loss}\")\n",
    "            for k, v in metric.items():\n",
    "                if k not in metrics:\n",
    "                    metrics[k] = torch.tensor(0.0)\n",
    "                metrics[k] += v / len(valid_loader)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    loss = sum(losses) / len(losses)\n",
    "\n",
    "    return loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_stereo import Logger\n",
    "from torch.cuda.amp import GradScaler\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    fusion_model.parameters(), lr=args.lr, weight_decay=args.wdecay, eps=1e-8\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    args.lr,\n",
    "    args.num_steps + 100,\n",
    "    pct_start=0.01,\n",
    "    cycle_momentum=False,\n",
    "    anneal_strategy=\"linear\",\n",
    ")\n",
    "\n",
    "total_steps = 0\n",
    "logger = Logger(fusion_model, scheduler)\n",
    "\n",
    "fusion_model.train()\n",
    "\n",
    "\n",
    "validation_frequency = 10000\n",
    "\n",
    "scaler = GradScaler(enabled=args.mixed_precision)\n",
    "\n",
    "should_keep_training = True\n",
    "global_batch_num = 0\n",
    "for param in raft_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "while should_keep_training:\n",
    "    raft_model.eval()\n",
    "    for i_batch, data_blob in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        image1, image2, image3, image4, depth = [x.cuda() for x in data_blob]\n",
    "\n",
    "        assert fusion_model.training\n",
    "        \n",
    "        image_fusion_1 = fusion_model(image1, image3)\n",
    "        image_fusion_2 = fusion_model(image2, image4)\n",
    "        \n",
    "        \n",
    "        assert fusion_model.training\n",
    "\n",
    "\n",
    "\n",
    "        loss, metrics = loss_fn((image_fusion_1, image_fusion_2), ((image1, image2), (image3, image4)), depth)\n",
    "        logger.writer.add_scalar(\"live_loss\", loss.item(), global_batch_num)\n",
    "        logger.writer.add_scalar(f'learning_rate', optimizer.param_groups[0]['lr'], global_batch_num)\n",
    "        global_batch_num += 1\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(fusion_model.parameters(), 1.0)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "\n",
    "        logger.push(metrics)\n",
    "\n",
    "        if total_steps % validation_frequency == validation_frequency - 1:\n",
    "            save_path = Path('checkpoints/%d_%s.pth' % (total_steps + 1, args.name))\n",
    "            logging.info(f\"Saving file {save_path.absolute()}\")\n",
    "            torch.save(fusion_model.state_dict(), save_path)\n",
    "\n",
    "            results = validate_things(fusion_model.module, valid_loader)\n",
    "\n",
    "            logger.write_dict(results)\n",
    "\n",
    "            fusion_model.train()\n",
    "\n",
    "\n",
    "        total_steps += 1\n",
    "\n",
    "        if total_steps > args.num_steps:\n",
    "            should_keep_training = False\n",
    "            break\n",
    "\n",
    "    if len(train_loader) >= 10000:\n",
    "        save_path = Path('checkpoints/%d_epoch_%s.pth.gz' % (total_steps + 1, args.name))\n",
    "        torch.save(fusion_model.state_dict(), save_path)\n",
    "\n",
    "print(\"FINISHED TRAINING\")\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "train_input = next(train_iter)\n",
    "fusion_model.eval()\n",
    "image1, image2, image3, image4, depth = [x.cuda() for x in train_input]\n",
    "with torch.no_grad():\n",
    "    image_fusion_1 = fusion_model(image1, image3)\n",
    "    image_fusion_2 = fusion_model(image2, image4)\n",
    "print(image_fusion_1.shape, image_fusion_1.max(), image3.shape)\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.subplot(131)\n",
    "plt.imshow(image1[0].permute(1,2,0).cpu().numpy().astype(np.uint8))\n",
    "plt.subplot(132)\n",
    "plt.imshow(image3[0].permute(1,2,0).cpu().numpy().astype(np.uint8), cmap=\"gray\")\n",
    "plt.subplot(133)\n",
    "plt.imshow(image_fusion_1[0].permute(1,2,0).cpu().numpy().astype(np.uint8))\n",
    "plt.show()\n",
    "with torch.no_grad():\n",
    "    disparity_rgb = -compute_disparity(image1, image2)\n",
    "    disparity_nir = -compute_disparity(image3, image4)\n",
    "    disparity_fusion = -compute_disparity(image_fusion_1, image_fusion_2)\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.subplot(131)\n",
    "plt.imshow(disparity_rgb[0,0].cpu().numpy())\n",
    "plt.subplot(132)\n",
    "plt.imshow(disparity_nir[0,0].cpu().numpy())\n",
    "plt.subplot(133)\n",
    "plt.imshow(disparity_fusion[0,0].cpu().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
